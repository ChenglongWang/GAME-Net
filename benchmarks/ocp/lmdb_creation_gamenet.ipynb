{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "controversial-lodge",
   "metadata": {},
   "source": [
    "# FG_Dataset lmdb creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b2ffd-e8a8-4bae-a61a-8c4f3b0c18ca",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7151a41-a9ca-4159-adc9-ad0c1065e440",
   "metadata": {},
   "source": [
    "This notebooks pack our data sets into the Open Catalysist Project lmdb format. If the `SPLIT_CV` option is selected, it will create multiple lmdb datasets by following the nested cross validation procedure depicted in our work.\n",
    "\n",
    "The dataset should have the following hierarchy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cad705-b74c-4d14-9473-ecc68b4db564",
   "metadata": {
    "tags": []
   },
   "source": [
    "## File Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235d8e6-f54a-4788-aad3-0888129c9a31",
   "metadata": {},
   "source": [
    "```\n",
    "├── Biomass\n",
    "│   └── structures\n",
    "│       ├── ni-mol5.contcar\n",
    "│       ├── ni-mol5.poscar\n",
    "│       ├── ru-mol1.poscar\n",
    "│       ├── ru-mol2.contcar\n",
    "│       ├── *.poscar\n",
    "│       └── *.contcar\n",
    "├── Plastics\n",
    "│   └── structures\n",
    "│       ├── *.poscar\n",
    "│       └── *.contcar\n",
    "├── Polyurethanes\n",
    "│   └── structures\n",
    "│       ├── *.poscar\n",
    "│       └── *.contcar\n",
    "├── energies.dat\n",
    "├── energies_i.dat\n",
    "└── groups.dat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e9a6d-6c7e-47e2-a998-a25fd346b57e",
   "metadata": {},
   "source": [
    "#### Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75317d1a-bdd0-462f-86f1-dc95db09a39c",
   "metadata": {},
   "source": [
    "Folders at root may represent the chemical group name of the geometries contained inside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823e66b-94ec-4447-a337-0e94211df392",
   "metadata": {},
   "source": [
    "#### Geometry Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb063ca-d449-41a4-8277-c9f1c09c9ba9",
   "metadata": {},
   "source": [
    "Initial structures should be labelled with the `.poscar` extension while final structures are labelled with the `.contcar` extension. \n",
    "The name of the file will match the sample label and should adhere to the following format:\n",
    "- `<metal>-<label>.(contcar|poscar)` for adsorbed structures, e.g. `ag-13X1-a.contcar`\n",
    "- `<metal>-0000.(contcar|poscar)` for metallic surfaces, e.g. `ag-0000.crontcar`\n",
    "- `<label>.(contcar|poscar)` for gasses, e.g. `49X6`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf5372-90af-48f1-ad2b-d28fc920d166",
   "metadata": {},
   "source": [
    "#### energies*.dat Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e6211-a612-4ba9-9d85-bd97eced6676",
   "metadata": {},
   "source": [
    "The energy files (energies.dat and energies_i.dat) should contain two colums, the former containing the names and the last containing the DFT energy. It is a good practice to include the surfaces energies, but they will be ignored during the lmdb packing. `energies.dat` contain DFT the energies obtained from the relaxed structure while `energies_i.dat` contain the energies obtained from the first converged SCF cycle of the relaxation (singlepoint calculation).\n",
    "\n",
    "Format example:\n",
    "```\n",
    "energies.dat\n",
    "\n",
    "49X6 -85.18458992\n",
    "49X7 -85.12457339\n",
    "49X8 -84.60803269\n",
    "ag-13X1-a -161.01991739\n",
    "ag-25X1-a -177.38788953\n",
    "ag-0000 -125.30436866\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fe47c-ac88-4a56-b07b-2fe11e5b61a9",
   "metadata": {},
   "source": [
    "#### groups.dat File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609b5c8-128c-4600-b13d-b5fa2fc47f83",
   "metadata": {},
   "source": [
    "`groups.dat` file follows the same structure as the `energies.dat` file, but instead of storing the DFT energies in the second column it stores group of the sample. If the name of the folders in root match the group names, the following shell command can be used to generate `groups.dat`:\n",
    "```bash\n",
    "for fn in ./*/structures/ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b9464-89c0-493a-a0cc-c933a81a83f8",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f25d8-b18f-4ae3-a4bc-4c4110ad6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path(\"./datasets/\")                 # Working directory\n",
    "DS = \"FG\"                                      # Either \"FG\" or \"BM\"\n",
    "\n",
    "TARBALL = ROOT_DIR/f\"{DS}_dataset_lite.tar.xz\" # Location of the dataset tarball\n",
    "#TARBALL = None                                # Set to False or None to avoid extraction.\n",
    "DS_DIR = ROOT_DIR/f\"{DS}_dataset_lite\"         # Dir of the initial Dataset\n",
    "DS_DIR_OUT = Path(f\"./lmdb/lmdb_{DS}\")         # Dir of the output dataset\n",
    "INITIAL_GEOMETRY = \"contcar\"                   # Either look for contcar or poscar files\n",
    "SPLIT_CV = { \"seed\": 42                        # Seed that will be used during the random splitting\n",
    "           , \"n_splits\": 5                     # Number of splits\n",
    "           , \"val_size\": 1                     # Number of splits in the validation set\n",
    "           , \"test_size\": 1 }                  # Number of splits in the test set\n",
    "#SPLIT_CV = None                               # Set it to False or None to avoid splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11b289-e6f5-4b07-aaa6-c95818c41eca",
   "metadata": {},
   "source": [
    "## Extract Tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac17fd-fe47-4599-9beb-1584d93db158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tarball to DS_DIR location\n",
    "if TARBALL:\n",
    "    import tarfile\n",
    "    tar_ds = tarfile.open(TARBALL, mode=\"r:xz\")\n",
    "    tar_ds.extractall(DS_DIR)\n",
    "    tar_ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab2dc0-6d36-449b-8e29-3c97fc9a2add",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d8d3b-88bd-4fdd-a8ca-576ab9a649c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read a file with two columns and transform it to a dictionary\n",
    "def read_two_columns(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return map(\n",
    "            lambda l: l.split()\n",
    "            , f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01710d4c-0e79-4747-bedf-e3205bf907c6",
   "metadata": {},
   "source": [
    "## Read structures and Energies\n",
    "\n",
    "Read structures, inital and final energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9904ceb-3ae2-4da9-91d5-4314169c3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ase.io.vasp import read_vasp, read_vasp_out\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "\n",
    "# Get energies in files\n",
    "iener_dict = dict(read_two_columns(DS_DIR/\"energies_i.dat\"))\n",
    "fener_dict = dict(read_two_columns(DS_DIR/\"energies.dat\"))\n",
    "\n",
    "def get_struct(fname):\n",
    "    final = read_vasp(fname)\n",
    "    final._calc = SinglePointCalculator(final, energy=float(fener_dict[fname.stem]))\n",
    "    return final                                \n",
    "                                                \n",
    "strct_map = map(\n",
    "    lambda d: (d.stem, get_struct(d))\n",
    "    , DS_DIR.glob(f\"./*/*/*.{INITIAL_GEOMETRY}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cbf39-9cc7-4fb5-af83-2c66bd51fd57",
   "metadata": {},
   "source": [
    "## Get Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035cb0e-42cb-4a55-a6f6-1a8cd1364d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import chain\n",
    "\n",
    "def reduce_grp(d, i):\n",
    "    match i:\n",
    "        case (k, v) if k in d: d[k].append(v)\n",
    "        case (k, v): d[k] = [v]\n",
    "    return d\n",
    "\n",
    "groups_direct = dict(read_two_columns(DS_DIR/\"groups.dat\"))\n",
    "groups_invert = map(\n",
    "    lambda xs: xs[::-1]\n",
    "    , groups_direct.items())\n",
    "\n",
    "groups_dict = reduce(\n",
    "    reduce_grp\n",
    "    , groups_invert\n",
    "    , {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30826e-5113-440a-a01a-85b5e7cef539",
   "metadata": {},
   "source": [
    "## Samples Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29285046-6e70-423b-9826-d7eb295f16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only final energies if contcar is selected\n",
    "if INITIAL_GEOMETRY == \"contcar\": \n",
    "    ener_pvt_dict = fener_dict\n",
    "else:\n",
    "    ener_pvt_dict = iener_dict\n",
    "    \n",
    "# Apply a filter to avoid collecting the metallic surfaces\n",
    "filter_surf = lambda x: \"0000\" not in x[0]\n",
    "\n",
    "ener_strct_map = map(\n",
    "    lambda x: (x[0], dict(name=x[0]\n",
    "                       , fener=float(fener_dict[x[0]])\n",
    "                       , iener=float(ener_pvt_dict[x[0]])\n",
    "                       , image=x[1]\n",
    "                       , group=groups_direct[x[0]]))\n",
    "    , filter(filter_surf, strct_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1346f-198c-474f-9dd8-aaae6a1863ad",
   "metadata": {},
   "source": [
    "## Extract Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3527cc-7572-4038-b335-2a25991dac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.preprocessing import AtomsToGraphs\n",
    "import torch\n",
    "\n",
    "a2g = AtomsToGraphs(\n",
    "    max_neigh=50,\n",
    "    radius=6,\n",
    "    r_energy=True,\n",
    "    r_forces=False,\n",
    "    r_distances=False,\n",
    "    r_fixed=True,\n",
    ")\n",
    "\n",
    "def read_entry_extract_features(a2g, strc):\n",
    "    tags = strc.get_tags()\n",
    "    data_objects = a2g.convert_all([strc], disable_tqdm=True)\n",
    "    data_objects[0].tags = torch.LongTensor(tags)\n",
    "    return data_objects\n",
    "\n",
    "def model_dict(xs):\n",
    "    idx = 0\n",
    "    out_dict = {}\n",
    "    for key, value in xs:\n",
    "        data_objects = read_entry_extract_features(a2g, value['image'])\n",
    "        init = data_objects[0]\n",
    "    \n",
    "        init.y_init = value[\"iener\"]\n",
    "        init.y_relaxed = init.y\n",
    "        del init.y\n",
    "        # As we are performing a IS2RE the final structure is not needed.\n",
    "        init.pos_relaxed = init.pos \n",
    "    \n",
    "        init.sid = idx\n",
    "        # Saving name and group for later identification.\n",
    "        init.name = value[\"name\"]\n",
    "        init.group = value[\"group\"]\n",
    "        \n",
    "        if init.edge_index.shape[1] == 0:\n",
    "            print(\"no neighbors\", idx)\n",
    "            continue\n",
    "        idx += 1\n",
    "        out_dict[key] = init\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c5751-22a6-4dc6-8d0a-7eb81ef2ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ase_dict = model_dict(ener_strct_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dc4ef-6a8c-4136-8adc-371ca49df395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Samples in the dataset: {len(ase_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb6807-2763-41c1-8960-70c349d6b2d4",
   "metadata": {},
   "source": [
    "## Process Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d323a-d03c-4783-aae8-a3d5322d9432",
   "metadata": {},
   "source": [
    "## Split Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8d8b2-e023-445c-a9cf-099569a42116",
   "metadata": {},
   "source": [
    "The algorithm below generates a nested cross validation set of sets equally splitting the dataset samples by chemical group.\n",
    "\n",
    "The behavior of the algorithm can be simplified as:\n",
    "1) Randomly shuffle the names in each group, using `seed` as the seed of the RNG.\n",
    "2) Split the samples of each chemical group into `n_splits` slices:\n",
    "    - `Dict[name, sample] -> Dict[group, [name]] -> n -> [[value]]`\n",
    "3) From these splits, generate the `n` unique combinations available by taking `val_size` slices as the validation set, `test_size` slices as the test set and the remaining as the training set: \n",
    "    - `[[value]] -> n -> n -> [([value], [value], [value])]`\n",
    "\n",
    "\n",
    "*Note: Step **1.** is impure and changes the order of the samples stored in `groups_dict`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798e3b0-b107-455a-92c1-243358e4fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ds(ase_dict, groups_dict, seed=42, n_splits=5, val_size=1, test_size=1):\n",
    "    from collections import deque\n",
    "    from itertools import chain, combinations, product\n",
    "    import random\n",
    "    from numpy import array_split\n",
    "    \n",
    "    random.seed(seed)\n",
    "\n",
    "    # Randomly shuffle the values stored in groups dict\n",
    "    deque(map(\n",
    "        random.shuffle\n",
    "        , groups_dict.values())\n",
    "        , maxlen=0)\n",
    "\n",
    "    # Filter structures that are in groups_dict but are not present\n",
    "    # in ase_dict\n",
    "    filtered_groups_dict = dict(map(\n",
    "        lambda xs: (xs[0]\n",
    "                    , tuple(filter(lambda x: x in ase_dict.keys()\n",
    "                             , xs[1])))\n",
    "        , groups_dict.items()))\n",
    "    \n",
    "    # Split the groups entries equally into n_splits slices\n",
    "    slices = reduce(\n",
    "        lambda l, t: map(lambda x: tuple(chain.from_iterable(x))\n",
    "                         , zip(l, t))\n",
    "        , map(lambda x: array_split(x, n_splits)\n",
    "            , filtered_groups_dict.values())\n",
    "        , [[]]*n_splits)\n",
    "\n",
    "    k_sets = set(map(\n",
    "        lambda x: tuple(map(ase_dict.get, x))\n",
    "        , slices))\n",
    "    \n",
    "    val_set = combinations(k_sets, val_size)\n",
    "    test_set = combinations(k_sets, test_size)\n",
    "    # Quick filter to discard combinations that lead to intersections between\n",
    "    # validation and test datasets.\n",
    "    val_test_comb = filter(\n",
    "        lambda xs: not set(xs[0]).intersection(set(xs[1]))\n",
    "        , product(val_set, test_set))\n",
    "    \n",
    "    # Chain the slices into training test and val\n",
    "    chain_n_tuple = lambda xs: tuple(chain.from_iterable(xs))\n",
    "    return map(\n",
    "        lambda xs: (chain_n_tuple(k_sets.difference(set(set(chain.from_iterable(xs)))))\n",
    "                    , chain_n_tuple(xs[0])\n",
    "                    , chain_n_tuple(xs[1]))\n",
    "        , val_test_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-haiti",
   "metadata": {},
   "source": [
    "## Write data to LMDB\n",
    "\n",
    "Write the three datasets into the lmdb format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073737ac-cbd4-49c0-a093-e50dc286b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def dump_db(xs, db_path):\n",
    "    db = lmdb.open(\n",
    "        str(db_path),\n",
    "        map_size=1099511627776 * 2,\n",
    "        subdir=False,\n",
    "        meminit=False,\n",
    "        map_async=True,\n",
    "    )\n",
    "    idx = 0\n",
    "    for value in xs:\n",
    "        txn = db.begin(write=True)\n",
    "        txn.put(f\"{idx}\".encode(\"ascii\"), pickle.dumps(value, protocol=-1))\n",
    "        txn.commit()\n",
    "        db.sync()\n",
    "        idx += 1\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3c0d8-1789-4773-b638-8704111ca8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os import makedirs\n",
    "\n",
    "mkdir_p = lambda p: p.is_dir() or makedirs(p)\n",
    "\n",
    "# Write three different lmdb for each of the splittings.\n",
    "if SPLIT_CV:\n",
    "    splitted_sets = split_ds(ase_dict, groups_dict, **SPLIT_CV)\n",
    "    for idx, n_set in enumerate(splitted_sets):\n",
    "        train, test, val = n_set\n",
    "        dpath = DS_DIR_OUT/str(idx)\n",
    "        mkdir_p(dpath)\n",
    "        dump_db(train, dpath/\"train.lmdb\")\n",
    "        dump_db(test, dpath/\"test.lmdb\")\n",
    "        dump_db(val, dpath/\"val.lmdb\")\n",
    "else:\n",
    "    dpath = DS_DIR_OUT\n",
    "    mkdir_p(dpath)\n",
    "    dump_db(ase_dict.values(), dpath/\"test.lmdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac0791-0526-4dd2-b48b-7f61926d339f",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e40b3e-b60b-43de-8fab-783054e566a7",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660af0bf-d303-4c23-a3cd-bf969f460ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ocpmodels.datasets import SinglePointLmdbDataset\n",
    "from pathlib import Path\n",
    "\n",
    "if SPLIT_CV:\n",
    "    target_glob = \"./*/*.lmdb\"\n",
    "else:\n",
    "    target_glob = \"./*.lmdb\"\n",
    "\n",
    "# Extract some useful metrics from the datasets\n",
    "# This step is not needed and can be done before without\n",
    "# reading the datasets again. However, errors can be\n",
    "# easily detected during this step and therefore, I use\n",
    "# it as a sanity check\n",
    "def get_metrics(lmdb_ds_path):\n",
    "    ds_arr = np.asarray(tuple(\n",
    "        map(\n",
    "            lambda x: x.y_relaxed\n",
    "            , SinglePointLmdbDataset({\"src\": str(lmdb_ds_path)})))\n",
    "        , dtype=float)\n",
    "    return {\n",
    "        \"mean\": np.mean(ds_arr)\n",
    "        , \"std\": np.std(ds_arr)\n",
    "        , \"idx\": lmdb_ds_path.parent.name\n",
    "        , \"split\": lmdb_ds_path.stem\n",
    "        , \"path\": lmdb_ds_path\n",
    "        , \"samples\": ds_arr.shape[0]\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame(map(\n",
    "    get_metrics\n",
    "    , DS_DIR_OUT.glob((\"./*.lmdb\", \"./*/*.lmdb\")[bool(SPLIT_CV)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ce07d-777e-48cd-accd-1dbe283775c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "deque(map(lambda xs: xs[1].to_csv( Path(xs[1][\"path\"].iloc[0]).parent/\"metrics.csv\"\n",
    "                            , header=False\n",
    "                            , index=False)\n",
    "    , metrics_df.groupby(\"idx\")[[\"split\", \"path\", \"mean\", \"std\"]]))\n",
    "\n",
    "print(f\"Written metrics for the ds in {DS_DIR_OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6573e9-458a-4aeb-b60d-c9d50eee169d",
   "metadata": {},
   "source": [
    "### Show Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e838ca6-f30b-442c-ab89-ae2de523eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
