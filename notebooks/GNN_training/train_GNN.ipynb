{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN training: Tutorial step by step\n",
    "\n",
    "Here we present the typical workflow applied to train GAME-Net. From the Graph FG-dataset generation to the training itself, with corersponding post-processing of the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "import time\n",
    "from os.path import exists\n",
    "import toml\n",
    "\n",
    "import torch\n",
    "\n",
    "from gnn_eads.constants import FG_RAW_GROUPS, sigma_dict, pool_dict, pool_seq_dict, conv_layer, loss_dict\n",
    "from gnn_eads.functions import create_loaders, scale_target, train_loop, test_loop, get_id\n",
    "from gnn_eads.processed_datasets import create_post_processed_datasets\n",
    "from gnn_eads.nets import FlexibleNet\n",
    "from gnn_eads.post_training import create_model_report\n",
    "from gnn_eads.create_graph_datasets import create_graph_datasets\n",
    "from gnn_eads.paths import create_paths\n",
    "from gnn_eads.graph_tools import plotter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load hyperparameters "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are the variables initialized before performing the model training (i.e., everything not trainable). Hyperparameters can be categorized into model-related and process-related: Model-related hyperparameters define the model architecture (e.g., layers' depth and width, bias, etc.), while the process-related ones define the training workflow (i.e., number of epochs, loss function, optimizer, batch size, etc.).\n",
    "\n",
    "The hyperparameters, together with the graph settings and the data path, are given as input via a .toml file. In the folder `input_train_GNN` a TEMPLATE.toml file is present. We will use this setting for this tutorial.\n",
    "\n",
    "P.S. Before loading the .toml file, open it and set you root folder where you store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = toml.load(\"input_train_GNN/TEMPLATE.toml\")  \n",
    "data_path = HYPERPARAMS[\"data\"][\"root\"]    \n",
    "graph_settings = HYPERPARAMS[\"graph\"]\n",
    "train = HYPERPARAMS[\"train\"]\n",
    "architecture = HYPERPARAMS[\"architecture\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create graphs from DFT data\n",
    "\n",
    "Based on the graph representation settings provided in the input .toml file (voronoi tolerance, metal scaling factor and 2nd-order metal atoms inclusion), the next cell will create the graph FG-dataset from the DFT data. This process involves two steps: \n",
    "1. Converting all the DFT data to graphs, which are saved as \"pre_xx_bool_yy.dat\" files in each FG-dataset subset family. These are plain text files which contain the necessary information to then generate the graph in a format suitable for Pytorch geometric.\n",
    "2. Generate the graph FG-dataset, processing the information in the pre_xx_bool_yy.dat files and filtering out wrong graph representations. The final FG_dataset object is a container of the chemical families in the FG-dataset. These are saved as \"post_xx_bool_yy.dat\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_identifier = get_id(graph_settings)\n",
    "family_paths = create_paths(FG_RAW_GROUPS, data_path, graph_identifier)\n",
    "if exists(data_path + \"/amides/pre_\" + graph_identifier):  \n",
    "    FG_dataset = create_post_processed_datasets(graph_identifier, family_paths)\n",
    "else:\n",
    "    print(\"Creating graphs from raw data ...\")  \n",
    "    create_graph_datasets(graph_settings, family_paths)\n",
    "    FG_dataset = create_post_processed_datasets(graph_identifier, family_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(FG_dataset)\n",
    "data_points = [len(FG_dataset[i]) for i in range(len(FG_dataset))]\n",
    "total_data_points = sum(data_points)\n",
    "print(\"Total number of data points: \", total_data_points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data Splitting and target scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FG-dataset is split among the train, validation and test sets via a stratified data split approach and then a target scaling is applied.\n",
    "The target scaling must be applied using parameters independent of the test set, as this would lead to \"data leakage\".\n",
    "Here, we apply the target scaling with the `scale_target` function, providing the optional parameter mode=\"std\" in order to apply standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_loaders(FG_dataset,\n",
    "                                                       batch_size=train[\"batch_size\"],\n",
    "                                                       split=train[\"splits\"], \n",
    "                                                       test=train[\"test_set\"])\n",
    "train_loader, val_loader, test_loader, mean, std = scale_target(train_loader,\n",
    "                                                                val_loader,\n",
    "                                                                test_loader,\n",
    "                                                                mode=train[\"target_scaling\"],\n",
    "                                                                test=train[\"test_set\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Graph inspection\n",
    "\n",
    "To have an idea of what the graphs objects are, here we show a visualization and the mathematical representation of a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph = train_loader.dataset[991]  # Change index to see different graphs\n",
    "plotter(random_graph, dpi=150) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is represented mathematically by (i) node feature matrix containing only the atomic element via one-hot encoding, (ii) the edge list which defined the connectvity and (iii) its scaled DFT scaled energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node atrtibutes\n",
    "random_graph.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connectivity\n",
    "random_graph.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph label\n",
    "random_graph.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Device selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a CUDA capable GPU is optimal for working with Deep Learning models, as its parallelized architecture can be exploited to speed up the training (i.e. huge number of matrix multiplications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_dict = {}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    print(\"Device name: {} (GPU)\".format(torch.cuda.get_device_name(0)))\n",
    "    device_dict[\"name\"] = torch.cuda.get_device_name(0)\n",
    "    device_dict[\"CudaDNN_enabled\"] = torch.backends.cudnn.enabled\n",
    "    device_dict[\"CUDNN_version\"] = torch.backends.cudnn.version()\n",
    "    device_dict[\"CUDA_version\"] = torch.version.cuda\n",
    "else:\n",
    "    print(\"Device name: CPU\")\n",
    "    device_dict[\"name\"] = \"CPU\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) GNN model instantiation\n",
    "\n",
    "Instantiate model object representing the graph neural network architecture and store it to the training device. We created the `FlexibleNet` class to build different architectures with the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlexibleNet(dim=architecture[\"dim\"],\n",
    "                        N_linear=architecture[\"n_linear\"], \n",
    "                        N_conv=architecture[\"n_conv\"], \n",
    "                        adj_conv=architecture[\"adj_conv\"],  \n",
    "                        sigma=sigma_dict[architecture[\"sigma\"]], \n",
    "                        bias=architecture[\"bias\"], \n",
    "                        conv=conv_layer[architecture[\"conv_layer\"]], \n",
    "                        pool=pool_dict[architecture[\"pool_layer\"]], \n",
    "                        pool_ratio=architecture[\"pool_ratio\"], \n",
    "                        pool_heads=architecture[\"pool_heads\"], \n",
    "                        pool_seq=pool_seq_dict[architecture[\"pool_seq\"]], \n",
    "                        pool_layer_norm=architecture[\"pool_layer_norm\"]).to(device)   \n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Define optimizer and learning rate scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used optimizer for the training is Adam, algorithm for first-order gradient-based optimization of\n",
    "stochastic objective functions, based on adaptive estimates of lower-order moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=train[\"lr0\"],\n",
    "                             eps=train[\"eps\"], \n",
    "                             weight_decay=train[\"weight_decay\"],\n",
    "                             amsgrad=train[\"amsgrad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps steering the learning rate during the training, providing faster convergence and higher accuracy. The used scheduler is the \"Reduce On Loss Plateau Decay\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                          mode='min',\n",
    "                                                          factor=train[\"factor\"],\n",
    "                                                          patience=train[\"patience\"],\n",
    "                                                          min_lr=train[\"minlr\"])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Run Training\n",
    "\n",
    "Everything is set up. Training a deep learning model is an iterative process, requiring multiple iterations (epochs) to make the model find the patterns found in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list, train_list, val_list, test_list, lr_list = [], [], [], [], []         \n",
    "t0 = time.time() \n",
    "for epoch in range(1, train[\"epochs\"]+1):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Update learning rate\n",
    "    lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "    # Train iteration        \n",
    "    loss, train_MAE = train_loop(model, device, train_loader, optimizer, loss_dict[train[\"loss_function\"]])  \n",
    "    # Validation iteration to update learning rate\n",
    "    val_MAE = test_loop(model, val_loader, device, std)  \n",
    "    lr_scheduler.step(val_MAE)\n",
    "    # Test iteration\n",
    "    if train[\"test_set\"]:\n",
    "        test_MAE = test_loop(model, test_loader, device, std, mean)         \n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.4f} eV  Validation MAE: {:.4f} eV '             \n",
    "              'Test MAE: {:.4f} eV'.format(epoch, lr, train_MAE*std, val_MAE, test_MAE))\n",
    "        test_list.append(test_MAE)\n",
    "    else:\n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.6f} eV  Validation MAE: {:.6f} eV '\n",
    "              .format(epoch, lr, train_MAE*std, val_MAE))  \n",
    "    # Save information       \n",
    "    loss_list.append(loss)\n",
    "    train_list.append(train_MAE * std)\n",
    "    val_list.append(val_MAE)\n",
    "    lr_list.append(lr)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "training_time = (time.time() - t0)/60  \n",
    "print(\"Training time: {:.2f} min\".format(training_time))\n",
    "device_dict[\"training_time\"] = training_time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Save model and performance analysis\n",
    "\n",
    "Depending on the use or not of a test set, the information stored in the model report folder will be different. If a test set is used to test the final model, more files will be generated (as learning curve, error distribution plot, etc.). \n",
    "The results are saved in the directory provided as second argument, in a folder called as the name provided as first argument to the function `create_model_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_report(\"TEMPLATE_test\",\n",
    "                    \"../../models\",\n",
    "                    HYPERPARAMS,\n",
    "                    model,\n",
    "                    (train_loader, val_loader, test_loader), \n",
    "                    (mean, std),  \n",
    "                    (train_list, val_list, test_list, lr_list), \n",
    "                    device_dict)                               "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
