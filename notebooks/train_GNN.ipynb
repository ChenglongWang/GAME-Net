{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkgutil import find_loader\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "from os.path import exists, isdir\n",
    "import toml\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "if not find_loader(\"gnn_eads\"):\n",
    "    sys.path.insert(0, \"../src/\")\n",
    "from gnn_eads.constants import FG_RAW_GROUPS, loss_dict, pool_seq_dict, conv_layer, sigma_dict, pool_dict\n",
    "from gnn_eads.functions import create_loaders, scale_target, train_loop, test_loop, get_id\n",
    "from gnn_eads.processed_datasets import create_post_processed_datasets\n",
    "from gnn_eads.nets import FlexibleNet\n",
    "from gnn_eads.post_training import create_model_report\n",
    "from gnn_eads.create_graph_datasets import create_paths, create_graph_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load hyperparameters of the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are all those parameters that are initialized before performing the model training (i.e., everything different from the model parameters). Hyperparameters can be categorized into model-related and process-related: Model-related hyperparameters are the activation function and the depth of the hidden layers, while the process-related ones are for example the batch size, the number of epochs and the loss function for the model optimization.\n",
    "\n",
    "The hyperparameters, together with the graph settings and the data path, are given as input via a toml file. In this folder are present a TEMPLATE.toml file and the configuration file referred to the best model presented in the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = toml.load(\"best_model.toml\")  \n",
    "data_path = HYPERPARAMS[\"data\"][\"root\"]    \n",
    "graph_settings = HYPERPARAMS[\"graph\"]\n",
    "train = HYPERPARAMS[\"train\"]\n",
    "architecture = HYPERPARAMS[\"architecture\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphs from raw DFT FG-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_identifier = get_id(graph_settings)\n",
    "family_paths = create_paths(FG_RAW_GROUPS, data_path, graph_identifier)\n",
    "if exists(data_path + \"/amides/pre_\" + graph_identifier):  \n",
    "    FG_dataset = create_post_processed_datasets(graph_identifier, family_paths)\n",
    "else:\n",
    "    print(\"Creating graphs from raw data ...\")  \n",
    "    create_graph_datasets(graph_settings, family_paths)\n",
    "    FG_dataset = create_post_processed_datasets(graph_identifier, family_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and target scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FG-dataset is split among the train, validation and test sets via a stratified data split approach.\n",
    "The target scaling must be applied using parameters independent of the test set, as this would lead to \"data leakage\".\n",
    "Here, we apply the target scaling with the `scale_target` function, providing the optional parameter mode=\"std\" in order to apply standardization. Normalization can be applied optionally, providing the parameter mode=\"norm\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_loaders(FG_dataset,\n",
    "                                                       batch_size=train[\"batch_size\"],\n",
    "                                                       split=train[\"splits\"], \n",
    "                                                       test=train[\"test_set\"])\n",
    "train_loader, val_loader, test_loader, mean, std = scale_target(train_loader,\n",
    "                                                                val_loader,\n",
    "                                                                test_loader,\n",
    "                                                                mode=train[\"target_scaling\"],\n",
    "                                                                test=train[\"test_set\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_eads.graph_tools import plotter\n",
    "plotter(train_loader.dataset[10]) # Random graph, change index to see other graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device selection (GPU/CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a CUDA capable GPU is optimal for working with Deep Learning models, as its structure can be exploited in order to speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_dict = {}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    print(\"Device name: {} (GPU)\".format(torch.cuda.get_device_name(0)))\n",
    "    device_dict[\"name\"] = torch.cuda.get_device_name(0)\n",
    "    device_dict[\"CudaDNN_enabled\"] = torch.backends.cudnn.enabled\n",
    "    device_dict[\"CUDNN_version\"] = torch.backends.cudnn.version()\n",
    "    device_dict[\"CUDA_version\"] = torch.version.cuda\n",
    "else:\n",
    "    print(\"Device name: CPU\")\n",
    "    device_dict[\"name\"] = \"CPU\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN model instantiation\n",
    "\n",
    "Instantiate model object and store it to the available device (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlexibleNet(dim=architecture[\"dim\"],\n",
    "                        N_linear=architecture[\"n_linear\"], \n",
    "                        N_conv=architecture[\"n_conv\"], \n",
    "                        adj_conv=architecture[\"adj_conv\"],  \n",
    "                        sigma=sigma_dict[architecture[\"sigma\"]], \n",
    "                        bias=architecture[\"bias\"], \n",
    "                        conv=conv_layer[architecture[\"conv_layer\"]], \n",
    "                        pool=pool_dict[architecture[\"pool_layer\"]], \n",
    "                        pool_ratio=architecture[\"pool_ratio\"], \n",
    "                        pool_heads=architecture[\"pool_heads\"], \n",
    "                        pool_seq=pool_seq_dict[architecture[\"pool_seq\"]], \n",
    "                        pool_layer_norm=architecture[\"pool_layer_norm\"]).to(device)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used optimizer for the training is Adam, algorithm for first-order gradient-based optimization of\n",
    "stochastic objective functions, based on adaptive estimates of lower-order mo-\n",
    "ments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=train[\"lr0\"],\n",
    "                                 eps=train[\"eps\"], \n",
    "                                 weight_decay=train[\"weight_decay\"],\n",
    "                                 amsgrad=train[\"amsgrad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate (LR) Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps steering the learning rate during the training, providing faster convergence and higher accuracy. The used scheduler is the \"Reduce On Loss Plateau Decay\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                              mode='min',\n",
    "                                                              factor=train[\"factor\"],\n",
    "                                                              patience=train[\"patience\"],\n",
    "                                                              min_lr=train[\"minlr\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list, train_list, val_list, test_list = [], [], [], []         \n",
    "t0 = time.time() \n",
    "for epoch in range(1, train[\"epochs\"]+1):\n",
    "    torch.cuda.empty_cache()\n",
    "    lr = lr_scheduler.optimizer.param_groups[0]['lr']        \n",
    "    loss, train_MAE = train_loop(model, device, train_loader, optimizer, loss_dict[train[\"loss_function\"]])  \n",
    "    val_MAE = test_loop(model, val_loader, device, std)  \n",
    "    lr_scheduler.step(val_MAE)\n",
    "    if train[\"test_set\"]:\n",
    "        test_MAE = test_loop(model, test_loader, device, std, mean)         \n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.4f} eV  Validation MAE: {:.4f} eV '             \n",
    "              'Test MAE: {:.4f} eV'.format(epoch, lr, train_MAE*std, val_MAE, test_MAE))\n",
    "        test_list.append(test_MAE)\n",
    "    else:\n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.6f} eV  Validation MAE: {:.6f} eV '\n",
    "              .format(epoch, lr, train_MAE*std, val_MAE))         \n",
    "    loss_list.append(loss)\n",
    "    train_list.append(train_MAE * std)\n",
    "    val_list.append(val_MAE)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "training_time = (time.time() - t0)/60  \n",
    "print(\"Training time: {:.2f} min\".format(training_time))\n",
    "device_dict[\"training_time\"] = training_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and performance analysis\n",
    "\n",
    "Depending on the use or not of a test set, the information stored in the model report folder will be different. If a test set is used to test the final model, more files will be generated (as learning curve, error distribution plot, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_report(\"TEST\",   # Provide a name different from models present in the directory \"models\"\n",
    "                    HYPERPARAMS,\n",
    "                    model,\n",
    "                    (train_loader, val_loader, test_loader), \n",
    "                    (mean, std),  \n",
    "                    (train_list, val_list, test_list), \n",
    "                    device_dict)\n",
    "                               "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "929eb757b548c43686da14be07befd842a942a01eb9dc843387956885175000a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('GNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
