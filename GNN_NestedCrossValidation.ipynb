{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n",
      "Device name: NVIDIA GeForce MX450\n",
      "CUDA Version: 11.3\n",
      "CuDNN Version: 8200\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import time\n",
    "from functions import *\n",
    "from graph_tools import *\n",
    "from processed_datasets import *\n",
    "from post_training import *\n",
    "from nets import SantyxNet\n",
    "from copy import copy, deepcopy\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "gnn_dataset = (group2_dataset, group2b_dataset,\n",
    "               aromatics_dataset, aromatics2_dataset,\n",
    "               amides_dataset, amidines_dataset,\n",
    "               oximes_dataset, carbamate_esters_dataset,\n",
    "               group3S_dataset, group3N_dataset,\n",
    "               group4_dataset, gas_amides_dataset,\n",
    "               gas_amidines_dataset, gas_aromatics_dataset,\n",
    "               gas_aromatics2_dataset, gas_carbamate_esters_dataset,\n",
    "               gas_group2_dataset, gas_group2b_dataset,\n",
    "               gas_group3N_dataset, gas_group3S_dataset,\n",
    "               gas_group4_dataset, gas_oximes_dataset) \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Current device: {}\".format(device))\n",
    "if device == \"cuda\":\n",
    "    print(\"Device name: {}\".format(torch.cuda.get_device_name(0)))\n",
    "    print(\"CUDA Version: {}\".format(torch.version.cuda))\n",
    "    print(\"CuDNN Version: {}\".format(torch.backends.cudnn.version()))\n",
    "    \n",
    "DIM = 128  \n",
    "EPOCHS = 300          \n",
    "LOSS_FN = F.l1_loss   \n",
    "BATCH_SIZE = 32       \n",
    "SPLITS = 5            \n",
    "LR = 0.001\n",
    "PATIENCE = 5   \n",
    "FACTOR = 0.7   \n",
    "MIN_LR = 1e-7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_list(a: list, n: int):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def create_loaders_nested(datasets, split=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create dataloaders for training+validation and test.\n",
    "    Args:\n",
    "        datasets(tuple): tuple containing the HetGraphDataset objects.\n",
    "        split(int): number of splits to generate train/val/test sets\n",
    "        batch(int): batch size    \n",
    "    Returns:\n",
    "        (tuple): tuple with dataloaders for training, validation and testing.\n",
    "    \"\"\"\n",
    "    chunk = [[] for _ in range(split)]\n",
    "    for dataset in datasets:\n",
    "        dataset.shuffle()\n",
    "        iterator = split_list(dataset, split)\n",
    "        for index, item in enumerate(iterator):\n",
    "            chunk[index] += item\n",
    "        chunk = sorted(chunk, key=len)\n",
    "    \n",
    "    for index in range(len(chunk)):\n",
    "        proxy = copy(chunk)\n",
    "        test_loader = DataLoader(proxy.pop(index), batch_size=batch_size, shuffle=False)\n",
    "        for index_2 in range(len(proxy)):\n",
    "            proxy_2 = copy(proxy)\n",
    "            val_loader = DataLoader(proxy_2.pop(index_2), batch_size=batch_size, shuffle=False)\n",
    "            flatten_training = [item for sublist in proxy_2 for item in sublist]\n",
    "            train_loader = DataLoader(flatten_training, batch_size=batch_size, shuffle=True)\n",
    "            yield deepcopy((train_loader, val_loader, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20-Epoch 001: LR=0.0010000  Loss=2.037068  Validation MAE: 12.030348 eV, Test MAE: 12.600851 eV\n",
      "1/20-Epoch 002: LR=0.0010000  Loss=0.466478  Validation MAE: 4.290116 eV, Test MAE: 4.918650 eV\n",
      "1/20-Epoch 003: LR=0.0010000  Loss=0.296394  Validation MAE: 10.934614 eV, Test MAE: 10.572973 eV\n",
      "1/20-Epoch 004: LR=0.0010000  Loss=0.313025  Validation MAE: 4.483588 eV, Test MAE: 4.766136 eV\n",
      "1/20-Epoch 005: LR=0.0010000  Loss=0.199345  Validation MAE: 4.228344 eV, Test MAE: 4.323762 eV\n",
      "1/20-Epoch 006: LR=0.0010000  Loss=0.163789  Validation MAE: 2.919768 eV, Test MAE: 2.983556 eV\n",
      "1/20-Epoch 007: LR=0.0010000  Loss=0.194935  Validation MAE: 2.335881 eV, Test MAE: 2.454028 eV\n",
      "1/20-Epoch 008: LR=0.0010000  Loss=0.174763  Validation MAE: 4.158808 eV, Test MAE: 4.004332 eV\n",
      "1/20-Epoch 009: LR=0.0010000  Loss=0.144839  Validation MAE: 2.202271 eV, Test MAE: 2.253823 eV\n",
      "1/20-Epoch 010: LR=0.0010000  Loss=0.152581  Validation MAE: 2.897738 eV, Test MAE: 2.777048 eV\n",
      "1/20-Epoch 011: LR=0.0010000  Loss=0.137166  Validation MAE: 2.015964 eV, Test MAE: 2.007656 eV\n",
      "1/20-Epoch 012: LR=0.0010000  Loss=0.231919  Validation MAE: 8.421665 eV, Test MAE: 8.623991 eV\n",
      "1/20-Epoch 013: LR=0.0010000  Loss=0.210390  Validation MAE: 2.528316 eV, Test MAE: 2.529005 eV\n",
      "1/20-Epoch 014: LR=0.0010000  Loss=0.104783  Validation MAE: 1.934248 eV, Test MAE: 1.894884 eV\n",
      "1/20-Epoch 015: LR=0.0010000  Loss=0.135940  Validation MAE: 2.865620 eV, Test MAE: 3.084870 eV\n",
      "1/20-Epoch 016: LR=0.0010000  Loss=0.116265  Validation MAE: 4.772155 eV, Test MAE: 4.662219 eV\n",
      "1/20-Epoch 017: LR=0.0010000  Loss=0.165720  Validation MAE: 1.542051 eV, Test MAE: 1.529901 eV\n",
      "1/20-Epoch 018: LR=0.0010000  Loss=0.091497  Validation MAE: 4.365669 eV, Test MAE: 4.526627 eV\n",
      "1/20-Epoch 019: LR=0.0010000  Loss=0.114492  Validation MAE: 3.185233 eV, Test MAE: 3.200855 eV\n",
      "1/20-Epoch 020: LR=0.0010000  Loss=0.115927  Validation MAE: 1.820867 eV, Test MAE: 1.767263 eV\n",
      "1/20-Epoch 021: LR=0.0010000  Loss=0.145787  Validation MAE: 2.105726 eV, Test MAE: 2.242645 eV\n",
      "1/20-Epoch 022: LR=0.0010000  Loss=0.127024  Validation MAE: 2.969118 eV, Test MAE: 2.845124 eV\n",
      "1/20-Epoch 023: LR=0.0010000  Loss=0.100454  Validation MAE: 1.769999 eV, Test MAE: 1.677966 eV\n",
      "1/20-Epoch 024: LR=0.0007000  Loss=0.110987  Validation MAE: 1.596615 eV, Test MAE: 1.536747 eV\n",
      "1/20-Epoch 025: LR=0.0007000  Loss=0.072751  Validation MAE: 1.584378 eV, Test MAE: 1.612455 eV\n",
      "1/20-Epoch 026: LR=0.0007000  Loss=0.064392  Validation MAE: 0.932179 eV, Test MAE: 0.994122 eV\n",
      "1/20-Epoch 027: LR=0.0007000  Loss=0.087233  Validation MAE: 1.118855 eV, Test MAE: 1.148558 eV\n",
      "1/20-Epoch 028: LR=0.0007000  Loss=0.054692  Validation MAE: 0.737234 eV, Test MAE: 0.790524 eV\n",
      "1/20-Epoch 029: LR=0.0007000  Loss=0.063668  Validation MAE: 1.232312 eV, Test MAE: 1.282255 eV\n",
      "1/20-Epoch 030: LR=0.0007000  Loss=0.073334  Validation MAE: 1.247524 eV, Test MAE: 1.262096 eV\n",
      "1/20-Epoch 031: LR=0.0007000  Loss=0.059990  Validation MAE: 0.803108 eV, Test MAE: 0.777269 eV\n",
      "1/20-Epoch 032: LR=0.0007000  Loss=0.046951  Validation MAE: 0.909907 eV, Test MAE: 0.946763 eV\n",
      "1/20-Epoch 033: LR=0.0007000  Loss=0.050450  Validation MAE: 1.463588 eV, Test MAE: 1.471055 eV\n",
      "1/20-Epoch 034: LR=0.0007000  Loss=0.057396  Validation MAE: 0.608353 eV, Test MAE: 0.623749 eV\n",
      "1/20-Epoch 035: LR=0.0007000  Loss=0.067507  Validation MAE: 1.138904 eV, Test MAE: 1.129337 eV\n",
      "1/20-Epoch 036: LR=0.0007000  Loss=0.055297  Validation MAE: 0.488632 eV, Test MAE: 0.519932 eV\n",
      "1/20-Epoch 037: LR=0.0007000  Loss=0.052997  Validation MAE: 0.784528 eV, Test MAE: 0.799951 eV\n",
      "1/20-Epoch 038: LR=0.0007000  Loss=0.068259  Validation MAE: 1.419627 eV, Test MAE: 1.478416 eV\n",
      "1/20-Epoch 039: LR=0.0007000  Loss=0.061750  Validation MAE: 1.740372 eV, Test MAE: 1.766676 eV\n",
      "1/20-Epoch 040: LR=0.0007000  Loss=0.075255  Validation MAE: 1.939012 eV, Test MAE: 1.970117 eV\n",
      "1/20-Epoch 041: LR=0.0007000  Loss=0.058260  Validation MAE: 0.983605 eV, Test MAE: 0.977914 eV\n",
      "1/20-Epoch 042: LR=0.0007000  Loss=0.072894  Validation MAE: 1.061270 eV, Test MAE: 1.081562 eV\n",
      "1/20-Epoch 043: LR=0.0004900  Loss=0.049656  Validation MAE: 0.382146 eV, Test MAE: 0.376301 eV\n",
      "1/20-Epoch 044: LR=0.0004900  Loss=0.035101  Validation MAE: 0.646594 eV, Test MAE: 0.663296 eV\n",
      "1/20-Epoch 045: LR=0.0004900  Loss=0.030002  Validation MAE: 0.496223 eV, Test MAE: 0.486970 eV\n",
      "1/20-Epoch 046: LR=0.0004900  Loss=0.031647  Validation MAE: 1.832146 eV, Test MAE: 1.841676 eV\n",
      "1/20-Epoch 047: LR=0.0004900  Loss=0.040813  Validation MAE: 0.734040 eV, Test MAE: 0.727340 eV\n",
      "1/20-Epoch 048: LR=0.0004900  Loss=0.033677  Validation MAE: 0.644894 eV, Test MAE: 0.668615 eV\n",
      "1/20-Epoch 049: LR=0.0004900  Loss=0.037097  Validation MAE: 0.823552 eV, Test MAE: 0.855956 eV\n",
      "1/20-Epoch 050: LR=0.0003430  Loss=0.039324  Validation MAE: 0.849060 eV, Test MAE: 0.829115 eV\n",
      "1/20-Epoch 051: LR=0.0003430  Loss=0.027782  Validation MAE: 0.378767 eV, Test MAE: 0.394599 eV\n",
      "1/20-Epoch 052: LR=0.0003430  Loss=0.027516  Validation MAE: 0.525651 eV, Test MAE: 0.530172 eV\n",
      "1/20-Epoch 053: LR=0.0003430  Loss=0.043433  Validation MAE: 0.345194 eV, Test MAE: 0.332320 eV\n",
      "1/20-Epoch 054: LR=0.0003430  Loss=0.030564  Validation MAE: 0.577809 eV, Test MAE: 0.597406 eV\n",
      "1/20-Epoch 055: LR=0.0003430  Loss=0.029073  Validation MAE: 0.506500 eV, Test MAE: 0.549166 eV\n",
      "1/20-Epoch 056: LR=0.0003430  Loss=0.035895  Validation MAE: 0.401058 eV, Test MAE: 0.403006 eV\n",
      "1/20-Epoch 057: LR=0.0003430  Loss=0.025416  Validation MAE: 0.834930 eV, Test MAE: 0.829334 eV\n",
      "1/20-Epoch 058: LR=0.0003430  Loss=0.056249  Validation MAE: 0.369517 eV, Test MAE: 0.372503 eV\n",
      "1/20-Epoch 059: LR=0.0003430  Loss=0.028117  Validation MAE: 0.389770 eV, Test MAE: 0.383614 eV\n",
      "1/20-Epoch 060: LR=0.0002401  Loss=0.018276  Validation MAE: 0.549007 eV, Test MAE: 0.510036 eV\n",
      "1/20-Epoch 061: LR=0.0002401  Loss=0.019955  Validation MAE: 0.522305 eV, Test MAE: 0.557910 eV\n",
      "1/20-Epoch 062: LR=0.0002401  Loss=0.025176  Validation MAE: 0.387455 eV, Test MAE: 0.370196 eV\n",
      "1/20-Epoch 063: LR=0.0002401  Loss=0.017457  Validation MAE: 0.485922 eV, Test MAE: 0.494076 eV\n",
      "1/20-Epoch 064: LR=0.0002401  Loss=0.020218  Validation MAE: 0.260043 eV, Test MAE: 0.269422 eV\n",
      "1/20-Epoch 065: LR=0.0002401  Loss=0.020652  Validation MAE: 0.283182 eV, Test MAE: 0.300150 eV\n",
      "1/20-Epoch 066: LR=0.0002401  Loss=0.018166  Validation MAE: 0.387200 eV, Test MAE: 0.414519 eV\n",
      "1/20-Epoch 067: LR=0.0002401  Loss=0.020476  Validation MAE: 0.306712 eV, Test MAE: 0.347559 eV\n",
      "1/20-Epoch 068: LR=0.0002401  Loss=0.017091  Validation MAE: 0.710079 eV, Test MAE: 0.678146 eV\n",
      "1/20-Epoch 069: LR=0.0002401  Loss=0.021381  Validation MAE: 0.824039 eV, Test MAE: 0.792332 eV\n",
      "1/20-Epoch 070: LR=0.0002401  Loss=0.029179  Validation MAE: 0.253749 eV, Test MAE: 0.248571 eV\n",
      "1/20-Epoch 071: LR=0.0002401  Loss=0.017069  Validation MAE: 0.336034 eV, Test MAE: 0.344057 eV\n",
      "1/20-Epoch 072: LR=0.0002401  Loss=0.019865  Validation MAE: 0.557357 eV, Test MAE: 0.554136 eV\n",
      "1/20-Epoch 073: LR=0.0002401  Loss=0.022849  Validation MAE: 0.225477 eV, Test MAE: 0.242275 eV\n",
      "1/20-Epoch 074: LR=0.0002401  Loss=0.020028  Validation MAE: 0.250742 eV, Test MAE: 0.245835 eV\n",
      "1/20-Epoch 075: LR=0.0002401  Loss=0.015265  Validation MAE: 0.705950 eV, Test MAE: 0.716960 eV\n",
      "1/20-Epoch 076: LR=0.0002401  Loss=0.026709  Validation MAE: 0.228897 eV, Test MAE: 0.231041 eV\n",
      "1/20-Epoch 077: LR=0.0002401  Loss=0.037034  Validation MAE: 1.032752 eV, Test MAE: 1.051341 eV\n",
      "1/20-Epoch 078: LR=0.0002401  Loss=0.029296  Validation MAE: 0.295033 eV, Test MAE: 0.300349 eV\n",
      "1/20-Epoch 079: LR=0.0002401  Loss=0.030539  Validation MAE: 0.557638 eV, Test MAE: 0.578914 eV\n",
      "1/20-Epoch 080: LR=0.0001681  Loss=0.016807  Validation MAE: 0.551646 eV, Test MAE: 0.553836 eV\n",
      "1/20-Epoch 081: LR=0.0001681  Loss=0.015091  Validation MAE: 0.237881 eV, Test MAE: 0.235646 eV\n",
      "1/20-Epoch 082: LR=0.0001681  Loss=0.014759  Validation MAE: 0.831990 eV, Test MAE: 0.851786 eV\n",
      "1/20-Epoch 083: LR=0.0001681  Loss=0.019080  Validation MAE: 0.302607 eV, Test MAE: 0.318214 eV\n",
      "1/20-Epoch 084: LR=0.0001681  Loss=0.022949  Validation MAE: 0.227202 eV, Test MAE: 0.226430 eV\n",
      "1/20-Epoch 085: LR=0.0001681  Loss=0.019811  Validation MAE: 0.301092 eV, Test MAE: 0.291539 eV\n",
      "1/20-Epoch 086: LR=0.0001176  Loss=0.015688  Validation MAE: 0.332994 eV, Test MAE: 0.349557 eV\n",
      "1/20-Epoch 087: LR=0.0001176  Loss=0.014312  Validation MAE: 0.478705 eV, Test MAE: 0.450332 eV\n",
      "1/20-Epoch 088: LR=0.0001176  Loss=0.014981  Validation MAE: 0.206761 eV, Test MAE: 0.213685 eV\n",
      "1/20-Epoch 089: LR=0.0001176  Loss=0.011146  Validation MAE: 0.267442 eV, Test MAE: 0.252205 eV\n",
      "1/20-Epoch 090: LR=0.0001176  Loss=0.011535  Validation MAE: 0.425929 eV, Test MAE: 0.404205 eV\n",
      "1/20-Epoch 091: LR=0.0001176  Loss=0.016037  Validation MAE: 0.218723 eV, Test MAE: 0.230463 eV\n",
      "1/20-Epoch 092: LR=0.0001176  Loss=0.015699  Validation MAE: 0.248307 eV, Test MAE: 0.233719 eV\n",
      "1/20-Epoch 093: LR=0.0001176  Loss=0.015351  Validation MAE: 0.287106 eV, Test MAE: 0.302956 eV\n",
      "1/20-Epoch 094: LR=0.0001176  Loss=0.013115  Validation MAE: 0.323562 eV, Test MAE: 0.311908 eV\n",
      "1/20-Epoch 095: LR=0.0000824  Loss=0.012099  Validation MAE: 0.243778 eV, Test MAE: 0.235655 eV\n",
      "1/20-Epoch 096: LR=0.0000824  Loss=0.011597  Validation MAE: 0.221823 eV, Test MAE: 0.220127 eV\n",
      "1/20-Epoch 097: LR=0.0000824  Loss=0.012003  Validation MAE: 0.255532 eV, Test MAE: 0.271162 eV\n",
      "1/20-Epoch 098: LR=0.0000824  Loss=0.012072  Validation MAE: 0.241176 eV, Test MAE: 0.246551 eV\n",
      "1/20-Epoch 099: LR=0.0000824  Loss=0.012629  Validation MAE: 0.310201 eV, Test MAE: 0.328821 eV\n",
      "1/20-Epoch 100: LR=0.0000824  Loss=0.011741  Validation MAE: 0.267481 eV, Test MAE: 0.280137 eV\n",
      "1/20-Epoch 101: LR=0.0000576  Loss=0.013197  Validation MAE: 0.212602 eV, Test MAE: 0.216210 eV\n",
      "1/20-Epoch 102: LR=0.0000576  Loss=0.012994  Validation MAE: 0.357249 eV, Test MAE: 0.349984 eV\n",
      "1/20-Epoch 103: LR=0.0000576  Loss=0.012623  Validation MAE: 0.205843 eV, Test MAE: 0.218093 eV\n",
      "1/20-Epoch 104: LR=0.0000576  Loss=0.010036  Validation MAE: 0.219433 eV, Test MAE: 0.210805 eV\n",
      "1/20-Epoch 105: LR=0.0000576  Loss=0.010760  Validation MAE: 0.194605 eV, Test MAE: 0.200424 eV\n",
      "1/20-Epoch 106: LR=0.0000576  Loss=0.009834  Validation MAE: 0.193661 eV, Test MAE: 0.195437 eV\n",
      "1/20-Epoch 107: LR=0.0000576  Loss=0.009793  Validation MAE: 0.197914 eV, Test MAE: 0.208003 eV\n",
      "1/20-Epoch 108: LR=0.0000576  Loss=0.009807  Validation MAE: 0.197035 eV, Test MAE: 0.196953 eV\n",
      "1/20-Epoch 109: LR=0.0000576  Loss=0.010820  Validation MAE: 0.447933 eV, Test MAE: 0.463086 eV\n",
      "1/20-Epoch 110: LR=0.0000576  Loss=0.013689  Validation MAE: 0.213253 eV, Test MAE: 0.205228 eV\n",
      "1/20-Epoch 111: LR=0.0000576  Loss=0.010813  Validation MAE: 0.206300 eV, Test MAE: 0.209883 eV\n",
      "1/20-Epoch 112: LR=0.0000576  Loss=0.010416  Validation MAE: 0.260487 eV, Test MAE: 0.282623 eV\n",
      "1/20-Epoch 113: LR=0.0000404  Loss=0.013023  Validation MAE: 0.194995 eV, Test MAE: 0.189214 eV\n",
      "1/20-Epoch 114: LR=0.0000404  Loss=0.010017  Validation MAE: 0.186128 eV, Test MAE: 0.192077 eV\n",
      "1/20-Epoch 115: LR=0.0000404  Loss=0.009706  Validation MAE: 0.223173 eV, Test MAE: 0.214178 eV\n",
      "1/20-Epoch 116: LR=0.0000404  Loss=0.009926  Validation MAE: 0.236491 eV, Test MAE: 0.225580 eV\n",
      "1/20-Epoch 117: LR=0.0000404  Loss=0.009650  Validation MAE: 0.203745 eV, Test MAE: 0.208383 eV\n",
      "1/20-Epoch 118: LR=0.0000404  Loss=0.009448  Validation MAE: 0.183515 eV, Test MAE: 0.184411 eV\n",
      "1/20-Epoch 119: LR=0.0000404  Loss=0.010544  Validation MAE: 0.347495 eV, Test MAE: 0.364875 eV\n",
      "1/20-Epoch 120: LR=0.0000404  Loss=0.012043  Validation MAE: 0.187385 eV, Test MAE: 0.188347 eV\n",
      "1/20-Epoch 121: LR=0.0000404  Loss=0.010379  Validation MAE: 0.187078 eV, Test MAE: 0.190474 eV\n",
      "1/20-Epoch 122: LR=0.0000404  Loss=0.009738  Validation MAE: 0.218083 eV, Test MAE: 0.209389 eV\n",
      "1/20-Epoch 123: LR=0.0000404  Loss=0.009787  Validation MAE: 0.183707 eV, Test MAE: 0.187079 eV\n",
      "1/20-Epoch 124: LR=0.0000404  Loss=0.009815  Validation MAE: 0.208835 eV, Test MAE: 0.215841 eV\n",
      "1/20-Epoch 125: LR=0.0000282  Loss=0.009107  Validation MAE: 0.180701 eV, Test MAE: 0.179978 eV\n",
      "1/20-Epoch 126: LR=0.0000282  Loss=0.008867  Validation MAE: 0.217026 eV, Test MAE: 0.210781 eV\n",
      "1/20-Epoch 127: LR=0.0000282  Loss=0.009307  Validation MAE: 0.225733 eV, Test MAE: 0.215645 eV\n",
      "1/20-Epoch 128: LR=0.0000282  Loss=0.008739  Validation MAE: 0.200549 eV, Test MAE: 0.192862 eV\n",
      "1/20-Epoch 129: LR=0.0000282  Loss=0.009244  Validation MAE: 0.188164 eV, Test MAE: 0.194802 eV\n",
      "1/20-Epoch 130: LR=0.0000282  Loss=0.009293  Validation MAE: 0.209248 eV, Test MAE: 0.201487 eV\n",
      "1/20-Epoch 131: LR=0.0000282  Loss=0.009473  Validation MAE: 0.187209 eV, Test MAE: 0.191589 eV\n",
      "1/20-Epoch 132: LR=0.0000198  Loss=0.009304  Validation MAE: 0.178386 eV, Test MAE: 0.175519 eV\n",
      "1/20-Epoch 133: LR=0.0000198  Loss=0.008982  Validation MAE: 0.210054 eV, Test MAE: 0.221714 eV\n",
      "1/20-Epoch 134: LR=0.0000198  Loss=0.008834  Validation MAE: 0.210849 eV, Test MAE: 0.219024 eV\n",
      "1/20-Epoch 135: LR=0.0000198  Loss=0.009423  Validation MAE: 0.182516 eV, Test MAE: 0.184077 eV\n",
      "1/20-Epoch 136: LR=0.0000198  Loss=0.008837  Validation MAE: 0.177796 eV, Test MAE: 0.178319 eV\n",
      "1/20-Epoch 137: LR=0.0000198  Loss=0.008719  Validation MAE: 0.181287 eV, Test MAE: 0.183059 eV\n",
      "1/20-Epoch 138: LR=0.0000198  Loss=0.008621  Validation MAE: 0.186663 eV, Test MAE: 0.191077 eV\n",
      "1/20-Epoch 139: LR=0.0000198  Loss=0.008685  Validation MAE: 0.188334 eV, Test MAE: 0.183874 eV\n",
      "1/20-Epoch 140: LR=0.0000198  Loss=0.009029  Validation MAE: 0.202110 eV, Test MAE: 0.196686 eV\n",
      "1/20-Epoch 141: LR=0.0000198  Loss=0.009116  Validation MAE: 0.185565 eV, Test MAE: 0.189297 eV\n",
      "1/20-Epoch 142: LR=0.0000198  Loss=0.008705  Validation MAE: 0.199046 eV, Test MAE: 0.194608 eV\n",
      "1/20-Epoch 143: LR=0.0000138  Loss=0.008591  Validation MAE: 0.185326 eV, Test MAE: 0.180222 eV\n",
      "1/20-Epoch 144: LR=0.0000138  Loss=0.008576  Validation MAE: 0.178273 eV, Test MAE: 0.176328 eV\n",
      "1/20-Epoch 145: LR=0.0000138  Loss=0.008796  Validation MAE: 0.180174 eV, Test MAE: 0.185333 eV\n",
      "1/20-Epoch 146: LR=0.0000138  Loss=0.008476  Validation MAE: 0.177234 eV, Test MAE: 0.176638 eV\n",
      "1/20-Epoch 147: LR=0.0000138  Loss=0.008538  Validation MAE: 0.179307 eV, Test MAE: 0.177271 eV\n",
      "1/20-Epoch 148: LR=0.0000138  Loss=0.008345  Validation MAE: 0.198365 eV, Test MAE: 0.207788 eV\n",
      "1/20-Epoch 149: LR=0.0000138  Loss=0.008820  Validation MAE: 0.185512 eV, Test MAE: 0.181862 eV\n",
      "1/20-Epoch 150: LR=0.0000138  Loss=0.008796  Validation MAE: 0.177174 eV, Test MAE: 0.178754 eV\n",
      "1/20-Epoch 151: LR=0.0000138  Loss=0.008747  Validation MAE: 0.176430 eV, Test MAE: 0.176351 eV\n",
      "1/20-Epoch 152: LR=0.0000138  Loss=0.008894  Validation MAE: 0.194226 eV, Test MAE: 0.188696 eV\n",
      "1/20-Epoch 153: LR=0.0000138  Loss=0.008724  Validation MAE: 0.180684 eV, Test MAE: 0.185348 eV\n",
      "1/20-Epoch 154: LR=0.0000138  Loss=0.008672  Validation MAE: 0.184940 eV, Test MAE: 0.179752 eV\n",
      "1/20-Epoch 155: LR=0.0000138  Loss=0.008490  Validation MAE: 0.178416 eV, Test MAE: 0.176135 eV\n",
      "1/20-Epoch 156: LR=0.0000138  Loss=0.009081  Validation MAE: 0.175969 eV, Test MAE: 0.178411 eV\n",
      "1/20-Epoch 157: LR=0.0000138  Loss=0.008875  Validation MAE: 0.188731 eV, Test MAE: 0.196069 eV\n",
      "1/20-Epoch 158: LR=0.0000138  Loss=0.008699  Validation MAE: 0.181430 eV, Test MAE: 0.177319 eV\n",
      "1/20-Epoch 159: LR=0.0000138  Loss=0.008742  Validation MAE: 0.176866 eV, Test MAE: 0.175867 eV\n",
      "1/20-Epoch 160: LR=0.0000138  Loss=0.009028  Validation MAE: 0.178145 eV, Test MAE: 0.175337 eV\n",
      "1/20-Epoch 161: LR=0.0000138  Loss=0.008331  Validation MAE: 0.176325 eV, Test MAE: 0.173052 eV\n",
      "1/20-Epoch 162: LR=0.0000138  Loss=0.008644  Validation MAE: 0.181159 eV, Test MAE: 0.178184 eV\n",
      "1/20-Epoch 163: LR=0.0000097  Loss=0.008251  Validation MAE: 0.177295 eV, Test MAE: 0.174597 eV\n",
      "1/20-Epoch 164: LR=0.0000097  Loss=0.008271  Validation MAE: 0.179697 eV, Test MAE: 0.183716 eV\n",
      "1/20-Epoch 165: LR=0.0000097  Loss=0.008561  Validation MAE: 0.174853 eV, Test MAE: 0.176530 eV\n",
      "1/20-Epoch 166: LR=0.0000097  Loss=0.008328  Validation MAE: 0.180580 eV, Test MAE: 0.178212 eV\n",
      "1/20-Epoch 167: LR=0.0000097  Loss=0.008172  Validation MAE: 0.177616 eV, Test MAE: 0.176860 eV\n",
      "1/20-Epoch 168: LR=0.0000097  Loss=0.008352  Validation MAE: 0.188749 eV, Test MAE: 0.196998 eV\n",
      "1/20-Epoch 169: LR=0.0000097  Loss=0.008845  Validation MAE: 0.176162 eV, Test MAE: 0.177724 eV\n",
      "1/20-Epoch 170: LR=0.0000097  Loss=0.008738  Validation MAE: 0.208831 eV, Test MAE: 0.200119 eV\n",
      "1/20-Epoch 171: LR=0.0000097  Loss=0.008497  Validation MAE: 0.173399 eV, Test MAE: 0.174001 eV\n",
      "1/20-Epoch 172: LR=0.0000097  Loss=0.008307  Validation MAE: 0.181388 eV, Test MAE: 0.176603 eV\n",
      "1/20-Epoch 173: LR=0.0000097  Loss=0.008094  Validation MAE: 0.185842 eV, Test MAE: 0.180272 eV\n",
      "1/20-Epoch 174: LR=0.0000097  Loss=0.008737  Validation MAE: 0.176546 eV, Test MAE: 0.178401 eV\n",
      "1/20-Epoch 175: LR=0.0000097  Loss=0.008655  Validation MAE: 0.190108 eV, Test MAE: 0.200682 eV\n",
      "1/20-Epoch 176: LR=0.0000097  Loss=0.008357  Validation MAE: 0.207905 eV, Test MAE: 0.198986 eV\n",
      "1/20-Epoch 177: LR=0.0000097  Loss=0.008396  Validation MAE: 0.175639 eV, Test MAE: 0.176085 eV\n",
      "1/20-Epoch 178: LR=0.0000068  Loss=0.008513  Validation MAE: 0.172996 eV, Test MAE: 0.172060 eV\n",
      "1/20-Epoch 179: LR=0.0000068  Loss=0.008204  Validation MAE: 0.174325 eV, Test MAE: 0.173819 eV\n",
      "1/20-Epoch 180: LR=0.0000068  Loss=0.008123  Validation MAE: 0.181036 eV, Test MAE: 0.176198 eV\n",
      "1/20-Epoch 181: LR=0.0000068  Loss=0.008187  Validation MAE: 0.179404 eV, Test MAE: 0.175368 eV\n",
      "1/20-Epoch 182: LR=0.0000068  Loss=0.008128  Validation MAE: 0.176443 eV, Test MAE: 0.174479 eV\n",
      "1/20-Epoch 183: LR=0.0000068  Loss=0.008123  Validation MAE: 0.174358 eV, Test MAE: 0.176681 eV\n",
      "1/20-Epoch 184: LR=0.0000068  Loss=0.008099  Validation MAE: 0.174112 eV, Test MAE: 0.173331 eV\n",
      "1/20-Epoch 185: LR=0.0000047  Loss=0.008029  Validation MAE: 0.172902 eV, Test MAE: 0.173511 eV\n",
      "1/20-Epoch 186: LR=0.0000047  Loss=0.008200  Validation MAE: 0.177598 eV, Test MAE: 0.178805 eV\n",
      "1/20-Epoch 187: LR=0.0000047  Loss=0.008138  Validation MAE: 0.174691 eV, Test MAE: 0.177830 eV\n",
      "1/20-Epoch 188: LR=0.0000047  Loss=0.008084  Validation MAE: 0.175013 eV, Test MAE: 0.172945 eV\n",
      "1/20-Epoch 189: LR=0.0000047  Loss=0.008099  Validation MAE: 0.172489 eV, Test MAE: 0.172241 eV\n",
      "1/20-Epoch 190: LR=0.0000047  Loss=0.008043  Validation MAE: 0.192792 eV, Test MAE: 0.185932 eV\n",
      "1/20-Epoch 191: LR=0.0000047  Loss=0.008177  Validation MAE: 0.173627 eV, Test MAE: 0.176378 eV\n",
      "1/20-Epoch 192: LR=0.0000047  Loss=0.008104  Validation MAE: 0.174974 eV, Test MAE: 0.172650 eV\n",
      "1/20-Epoch 193: LR=0.0000047  Loss=0.008148  Validation MAE: 0.182878 eV, Test MAE: 0.177932 eV\n",
      "1/20-Epoch 194: LR=0.0000047  Loss=0.008079  Validation MAE: 0.171888 eV, Test MAE: 0.171767 eV\n",
      "1/20-Epoch 195: LR=0.0000047  Loss=0.008198  Validation MAE: 0.173166 eV, Test MAE: 0.174297 eV\n",
      "1/20-Epoch 196: LR=0.0000047  Loss=0.008113  Validation MAE: 0.173035 eV, Test MAE: 0.172313 eV\n",
      "1/20-Epoch 197: LR=0.0000047  Loss=0.008011  Validation MAE: 0.172200 eV, Test MAE: 0.173361 eV\n",
      "1/20-Epoch 198: LR=0.0000047  Loss=0.008087  Validation MAE: 0.171639 eV, Test MAE: 0.173331 eV\n",
      "1/20-Epoch 199: LR=0.0000047  Loss=0.008013  Validation MAE: 0.176816 eV, Test MAE: 0.182606 eV\n",
      "1/20-Epoch 200: LR=0.0000047  Loss=0.008171  Validation MAE: 0.172019 eV, Test MAE: 0.173541 eV\n",
      "1/20-Epoch 201: LR=0.0000047  Loss=0.008066  Validation MAE: 0.171580 eV, Test MAE: 0.173322 eV\n",
      "1/20-Epoch 202: LR=0.0000047  Loss=0.008170  Validation MAE: 0.173362 eV, Test MAE: 0.173475 eV\n",
      "1/20-Epoch 203: LR=0.0000047  Loss=0.008039  Validation MAE: 0.171763 eV, Test MAE: 0.173376 eV\n",
      "1/20-Epoch 204: LR=0.0000047  Loss=0.008076  Validation MAE: 0.177818 eV, Test MAE: 0.175338 eV\n",
      "1/20-Epoch 205: LR=0.0000047  Loss=0.008023  Validation MAE: 0.189857 eV, Test MAE: 0.184323 eV\n",
      "1/20-Epoch 206: LR=0.0000047  Loss=0.008155  Validation MAE: 0.171738 eV, Test MAE: 0.173477 eV\n",
      "1/20-Epoch 207: LR=0.0000047  Loss=0.008178  Validation MAE: 0.172755 eV, Test MAE: 0.174675 eV\n",
      "1/20-Epoch 208: LR=0.0000033  Loss=0.008052  Validation MAE: 0.172724 eV, Test MAE: 0.174863 eV\n",
      "1/20-Epoch 209: LR=0.0000033  Loss=0.007946  Validation MAE: 0.180284 eV, Test MAE: 0.178493 eV\n",
      "1/20-Epoch 210: LR=0.0000033  Loss=0.008006  Validation MAE: 0.174380 eV, Test MAE: 0.171826 eV\n",
      "1/20-Epoch 211: LR=0.0000033  Loss=0.007980  Validation MAE: 0.173975 eV, Test MAE: 0.177016 eV\n",
      "1/20-Epoch 212: LR=0.0000033  Loss=0.008007  Validation MAE: 0.172609 eV, Test MAE: 0.171340 eV\n",
      "1/20-Epoch 213: LR=0.0000033  Loss=0.008075  Validation MAE: 0.173736 eV, Test MAE: 0.177162 eV\n",
      "1/20-Epoch 214: LR=0.0000023  Loss=0.008065  Validation MAE: 0.171461 eV, Test MAE: 0.172682 eV\n",
      "1/20-Epoch 215: LR=0.0000023  Loss=0.007901  Validation MAE: 0.171653 eV, Test MAE: 0.171314 eV\n",
      "1/20-Epoch 216: LR=0.0000023  Loss=0.007847  Validation MAE: 0.172569 eV, Test MAE: 0.172058 eV\n",
      "1/20-Epoch 217: LR=0.0000023  Loss=0.007884  Validation MAE: 0.173222 eV, Test MAE: 0.171889 eV\n",
      "1/20-Epoch 218: LR=0.0000023  Loss=0.008017  Validation MAE: 0.173335 eV, Test MAE: 0.171006 eV\n",
      "1/20-Epoch 219: LR=0.0000023  Loss=0.007934  Validation MAE: 0.175669 eV, Test MAE: 0.172402 eV\n",
      "1/20-Epoch 220: LR=0.0000023  Loss=0.007989  Validation MAE: 0.172395 eV, Test MAE: 0.170908 eV\n",
      "1/20-Epoch 221: LR=0.0000016  Loss=0.007889  Validation MAE: 0.171784 eV, Test MAE: 0.172744 eV\n",
      "1/20-Epoch 222: LR=0.0000016  Loss=0.007857  Validation MAE: 0.170922 eV, Test MAE: 0.170745 eV\n",
      "1/20-Epoch 223: LR=0.0000016  Loss=0.007840  Validation MAE: 0.171001 eV, Test MAE: 0.171951 eV\n",
      "1/20-Epoch 224: LR=0.0000016  Loss=0.007925  Validation MAE: 0.173031 eV, Test MAE: 0.171159 eV\n",
      "1/20-Epoch 225: LR=0.0000016  Loss=0.007854  Validation MAE: 0.176909 eV, Test MAE: 0.173977 eV\n",
      "1/20-Epoch 226: LR=0.0000016  Loss=0.007893  Validation MAE: 0.172196 eV, Test MAE: 0.170341 eV\n",
      "1/20-Epoch 227: LR=0.0000016  Loss=0.007864  Validation MAE: 0.171354 eV, Test MAE: 0.170266 eV\n",
      "1/20-Epoch 228: LR=0.0000016  Loss=0.007949  Validation MAE: 0.170608 eV, Test MAE: 0.171120 eV\n",
      "1/20-Epoch 229: LR=0.0000016  Loss=0.007846  Validation MAE: 0.173108 eV, Test MAE: 0.171238 eV\n",
      "1/20-Epoch 230: LR=0.0000016  Loss=0.007874  Validation MAE: 0.170672 eV, Test MAE: 0.170420 eV\n",
      "1/20-Epoch 231: LR=0.0000016  Loss=0.007834  Validation MAE: 0.171019 eV, Test MAE: 0.170719 eV\n",
      "1/20-Epoch 232: LR=0.0000016  Loss=0.007854  Validation MAE: 0.172851 eV, Test MAE: 0.171054 eV\n",
      "1/20-Epoch 233: LR=0.0000016  Loss=0.007875  Validation MAE: 0.174063 eV, Test MAE: 0.171557 eV\n",
      "1/20-Epoch 234: LR=0.0000016  Loss=0.007849  Validation MAE: 0.170364 eV, Test MAE: 0.171040 eV\n",
      "1/20-Epoch 235: LR=0.0000016  Loss=0.007837  Validation MAE: 0.170089 eV, Test MAE: 0.170838 eV\n",
      "1/20-Epoch 236: LR=0.0000016  Loss=0.007839  Validation MAE: 0.169698 eV, Test MAE: 0.170502 eV\n",
      "1/20-Epoch 237: LR=0.0000016  Loss=0.007827  Validation MAE: 0.172885 eV, Test MAE: 0.172403 eV\n",
      "1/20-Epoch 238: LR=0.0000016  Loss=0.007912  Validation MAE: 0.169910 eV, Test MAE: 0.171275 eV\n",
      "1/20-Epoch 239: LR=0.0000016  Loss=0.007941  Validation MAE: 0.170431 eV, Test MAE: 0.172324 eV\n",
      "1/20-Epoch 240: LR=0.0000016  Loss=0.007918  Validation MAE: 0.170204 eV, Test MAE: 0.170009 eV\n",
      "1/20-Epoch 241: LR=0.0000016  Loss=0.007784  Validation MAE: 0.169738 eV, Test MAE: 0.169772 eV\n",
      "1/20-Epoch 242: LR=0.0000016  Loss=0.007825  Validation MAE: 0.171801 eV, Test MAE: 0.171928 eV\n",
      "1/20-Epoch 243: LR=0.0000011  Loss=0.007777  Validation MAE: 0.169900 eV, Test MAE: 0.170444 eV\n",
      "1/20-Epoch 244: LR=0.0000011  Loss=0.007810  Validation MAE: 0.169830 eV, Test MAE: 0.170211 eV\n",
      "1/20-Epoch 245: LR=0.0000011  Loss=0.007809  Validation MAE: 0.169970 eV, Test MAE: 0.169479 eV\n",
      "1/20-Epoch 246: LR=0.0000011  Loss=0.007772  Validation MAE: 0.169509 eV, Test MAE: 0.170358 eV\n",
      "1/20-Epoch 247: LR=0.0000011  Loss=0.007832  Validation MAE: 0.170423 eV, Test MAE: 0.170063 eV\n",
      "1/20-Epoch 248: LR=0.0000011  Loss=0.007754  Validation MAE: 0.169499 eV, Test MAE: 0.169756 eV\n",
      "1/20-Epoch 249: LR=0.0000011  Loss=0.007807  Validation MAE: 0.169555 eV, Test MAE: 0.170043 eV\n",
      "1/20-Epoch 250: LR=0.0000011  Loss=0.007835  Validation MAE: 0.169473 eV, Test MAE: 0.169756 eV\n",
      "1/20-Epoch 251: LR=0.0000011  Loss=0.007757  Validation MAE: 0.170117 eV, Test MAE: 0.172017 eV\n",
      "1/20-Epoch 252: LR=0.0000011  Loss=0.007842  Validation MAE: 0.172579 eV, Test MAE: 0.170478 eV\n",
      "1/20-Epoch 253: LR=0.0000011  Loss=0.007751  Validation MAE: 0.169657 eV, Test MAE: 0.170340 eV\n",
      "1/20-Epoch 254: LR=0.0000011  Loss=0.007840  Validation MAE: 0.169487 eV, Test MAE: 0.169968 eV\n",
      "1/20-Epoch 255: LR=0.0000011  Loss=0.007747  Validation MAE: 0.170363 eV, Test MAE: 0.170286 eV\n",
      "1/20-Epoch 256: LR=0.0000011  Loss=0.007767  Validation MAE: 0.169736 eV, Test MAE: 0.169534 eV\n",
      "1/20-Epoch 257: LR=0.0000008  Loss=0.007754  Validation MAE: 0.170006 eV, Test MAE: 0.169964 eV\n",
      "1/20-Epoch 258: LR=0.0000008  Loss=0.007736  Validation MAE: 0.171685 eV, Test MAE: 0.170112 eV\n",
      "1/20-Epoch 259: LR=0.0000008  Loss=0.007788  Validation MAE: 0.170434 eV, Test MAE: 0.169580 eV\n",
      "1/20-Epoch 260: LR=0.0000008  Loss=0.007738  Validation MAE: 0.169455 eV, Test MAE: 0.169755 eV\n",
      "1/20-Epoch 261: LR=0.0000008  Loss=0.007764  Validation MAE: 0.169382 eV, Test MAE: 0.170847 eV\n",
      "1/20-Epoch 262: LR=0.0000008  Loss=0.007750  Validation MAE: 0.169502 eV, Test MAE: 0.170529 eV\n",
      "1/20-Epoch 263: LR=0.0000008  Loss=0.007731  Validation MAE: 0.170310 eV, Test MAE: 0.169248 eV\n",
      "1/20-Epoch 264: LR=0.0000008  Loss=0.007712  Validation MAE: 0.169611 eV, Test MAE: 0.169343 eV\n",
      "1/20-Epoch 265: LR=0.0000008  Loss=0.007713  Validation MAE: 0.169548 eV, Test MAE: 0.171111 eV\n",
      "1/20-Epoch 266: LR=0.0000008  Loss=0.007756  Validation MAE: 0.169347 eV, Test MAE: 0.170182 eV\n",
      "1/20-Epoch 267: LR=0.0000008  Loss=0.007709  Validation MAE: 0.170092 eV, Test MAE: 0.169696 eV\n",
      "1/20-Epoch 268: LR=0.0000008  Loss=0.007716  Validation MAE: 0.169490 eV, Test MAE: 0.171136 eV\n",
      "1/20-Epoch 269: LR=0.0000008  Loss=0.007730  Validation MAE: 0.170677 eV, Test MAE: 0.169680 eV\n",
      "1/20-Epoch 270: LR=0.0000008  Loss=0.007727  Validation MAE: 0.169926 eV, Test MAE: 0.170542 eV\n",
      "1/20-Epoch 271: LR=0.0000008  Loss=0.007765  Validation MAE: 0.169827 eV, Test MAE: 0.169589 eV\n",
      "1/20-Epoch 272: LR=0.0000008  Loss=0.007770  Validation MAE: 0.169877 eV, Test MAE: 0.169293 eV\n",
      "1/20-Epoch 273: LR=0.0000006  Loss=0.007756  Validation MAE: 0.169723 eV, Test MAE: 0.169881 eV\n",
      "1/20-Epoch 274: LR=0.0000006  Loss=0.007713  Validation MAE: 0.169251 eV, Test MAE: 0.169696 eV\n",
      "1/20-Epoch 275: LR=0.0000006  Loss=0.007719  Validation MAE: 0.169887 eV, Test MAE: 0.169956 eV\n",
      "1/20-Epoch 276: LR=0.0000006  Loss=0.007720  Validation MAE: 0.169165 eV, Test MAE: 0.170107 eV\n",
      "1/20-Epoch 277: LR=0.0000006  Loss=0.007765  Validation MAE: 0.169228 eV, Test MAE: 0.170064 eV\n",
      "1/20-Epoch 278: LR=0.0000006  Loss=0.007711  Validation MAE: 0.169455 eV, Test MAE: 0.169049 eV\n",
      "1/20-Epoch 279: LR=0.0000006  Loss=0.007746  Validation MAE: 0.172921 eV, Test MAE: 0.171012 eV\n",
      "1/20-Epoch 280: LR=0.0000006  Loss=0.007742  Validation MAE: 0.170872 eV, Test MAE: 0.169585 eV\n",
      "1/20-Epoch 281: LR=0.0000006  Loss=0.007744  Validation MAE: 0.169339 eV, Test MAE: 0.169258 eV\n",
      "1/20-Epoch 282: LR=0.0000006  Loss=0.007732  Validation MAE: 0.169325 eV, Test MAE: 0.169656 eV\n",
      "1/20-Epoch 283: LR=0.0000004  Loss=0.007728  Validation MAE: 0.169945 eV, Test MAE: 0.169469 eV\n",
      "1/20-Epoch 284: LR=0.0000004  Loss=0.007703  Validation MAE: 0.169335 eV, Test MAE: 0.169410 eV\n",
      "1/20-Epoch 285: LR=0.0000004  Loss=0.007704  Validation MAE: 0.169998 eV, Test MAE: 0.169353 eV\n",
      "1/20-Epoch 286: LR=0.0000004  Loss=0.007703  Validation MAE: 0.169364 eV, Test MAE: 0.169335 eV\n",
      "1/20-Epoch 287: LR=0.0000004  Loss=0.007687  Validation MAE: 0.170040 eV, Test MAE: 0.169383 eV\n",
      "1/20-Epoch 288: LR=0.0000004  Loss=0.007683  Validation MAE: 0.170499 eV, Test MAE: 0.169506 eV\n",
      "1/20-Epoch 289: LR=0.0000003  Loss=0.007712  Validation MAE: 0.169349 eV, Test MAE: 0.169680 eV\n",
      "1/20-Epoch 290: LR=0.0000003  Loss=0.007688  Validation MAE: 0.169148 eV, Test MAE: 0.169951 eV\n",
      "1/20-Epoch 291: LR=0.0000003  Loss=0.007713  Validation MAE: 0.169626 eV, Test MAE: 0.169340 eV\n",
      "1/20-Epoch 292: LR=0.0000003  Loss=0.007681  Validation MAE: 0.170841 eV, Test MAE: 0.169644 eV\n",
      "1/20-Epoch 293: LR=0.0000003  Loss=0.007695  Validation MAE: 0.169125 eV, Test MAE: 0.169745 eV\n",
      "1/20-Epoch 294: LR=0.0000003  Loss=0.007684  Validation MAE: 0.169769 eV, Test MAE: 0.169371 eV\n",
      "1/20-Epoch 295: LR=0.0000003  Loss=0.007691  Validation MAE: 0.169234 eV, Test MAE: 0.169526 eV\n",
      "1/20-Epoch 296: LR=0.0000003  Loss=0.007701  Validation MAE: 0.169229 eV, Test MAE: 0.169433 eV\n",
      "1/20-Epoch 297: LR=0.0000003  Loss=0.007686  Validation MAE: 0.169388 eV, Test MAE: 0.169362 eV\n",
      "1/20-Epoch 298: LR=0.0000003  Loss=0.007692  Validation MAE: 0.172113 eV, Test MAE: 0.169911 eV\n",
      "1/20-Epoch 299: LR=0.0000003  Loss=0.007766  Validation MAE: 0.169850 eV, Test MAE: 0.169331 eV\n",
      "1/20-Epoch 300: LR=0.0000002  Loss=0.007696  Validation MAE: 0.169955 eV, Test MAE: 0.169606 eV\n",
      "Training time: 391.05 s\n",
      "01) Outlier Detected: C2H2-Ag1          Error: -0.98 eV    (index=13)\n",
      "02) Outlier Detected: C4H6-Ag1          Error: -0.87 eV    (index=42)\n",
      "03) Outlier Detected: C6H6-Rh7          Error: -2.45 eV    (index=70)\n",
      "04) Outlier Detected: C3H7N1O1-Ag1      Error: 1.01 eV    (index=146)\n",
      "05) Outlier Detected: C5H6-(g)          Error: -1.05 eV    (index=473)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.83 GiB total capacity; 80.82 MiB already allocated; 45.81 MiB free; 88.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/santiago/Desktop/GNN/GNN_NestedCrossValidation.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/GNN_NestedCrossValidation.ipynb#ch0000002?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/GNN_NestedCrossValidation.ipynb#ch0000002?line=20'>21</a>\u001b[0m     lr \u001b[39m=\u001b[39m scheduler\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/GNN_NestedCrossValidation.ipynb#ch0000002?line=21'>22</a>\u001b[0m     loss, train_MAE \u001b[39m=\u001b[39m train_loop(model, device, train_loader, optimizer, LOSS_FN)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/GNN_NestedCrossValidation.ipynb#ch0000002?line=22'>23</a>\u001b[0m     val_MAE \u001b[39m=\u001b[39m test_loop(model, val_loader, device, std_tv)            \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/GNN_NestedCrossValidation.ipynb#ch0000002?line=23'>24</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(val_MAE)                                                       \n",
      "File \u001b[0;32m~/Desktop/GNN/functions.py:481\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, device, train_loader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/functions.py?line=478'>479</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()                     \u001b[39m# Set gradients of all tensors to zero\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/functions.py?line=479'>480</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(model(batch), batch\u001b[39m.\u001b[39my)\n\u001b[0;32m--> <a href='file:///home/santiago/Desktop/GNN/functions.py?line=480'>481</a>\u001b[0m mae \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39ml1_loss(model(batch), batch\u001b[39m.\u001b[39my)    \u001b[39m# For comparison with val/test data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/functions.py?line=481'>482</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()                           \u001b[39m# Compute gradient of loss function wrt parameters\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/functions.py?line=482'>483</a>\u001b[0m loss_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch\u001b[39m.\u001b[39mnum_graphs\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/GNN/nets.py:129\u001b[0m, in \u001b[0;36mSantyxNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/nets.py?line=124'>125</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin6(out))\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/nets.py?line=125'>126</a>\u001b[0m \u001b[39m#----------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/nets.py?line=126'>127</a>\u001b[0m \u001b[39m# GRAPH LEVEL (POOLING)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/nets.py?line=127'>128</a>\u001b[0m \u001b[39m#----------------------\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/santiago/Desktop/GNN/nets.py?line=128'>129</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool(out, data\u001b[39m.\u001b[39;49mbatch, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[1;32m    <a href='file:///home/santiago/Desktop/GNN/nets.py?line=129'>130</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py:240\u001b[0m, in \u001b[0;36mGraphMultisetTransformer.forward\u001b[0;34m(self, x, batch, edge_index)\u001b[0m\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=237'>238</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (name, pool) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_sequences, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpools)):\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=238'>239</a>\u001b[0m     graph \u001b[39m=\u001b[39m (x, edge_index, batch) \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mGMPool_G\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=239'>240</a>\u001b[0m     batch_x \u001b[39m=\u001b[39m pool(batch_x, graph, mask)\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=240'>241</a>\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=242'>243</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin2(batch_x\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py:107\u001b[0m, in \u001b[0;36mSAB.forward\u001b[0;34m(self, x, graph, mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=100'>101</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=101'>102</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=102'>103</a>\u001b[0m     x: Tensor,\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=103'>104</a>\u001b[0m     graph: Optional[Tuple[Tensor, Tensor, Tensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=104'>105</a>\u001b[0m     mask: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=105'>106</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=106'>107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmab(x, x, graph, mask)\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py:76\u001b[0m, in \u001b[0;36mMAB.forward\u001b[0;34m(self, Q, K, graph, mask)\u001b[0m\n\u001b[1;32m     <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=72'>73</a>\u001b[0m     A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(mask \u001b[39m+\u001b[39m attention_score, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=73'>74</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=74'>75</a>\u001b[0m     A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(\n\u001b[0;32m---> <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=75'>76</a>\u001b[0m         Q_\u001b[39m.\u001b[39;49mbmm(K_\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)) \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim_V), \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=77'>78</a>\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((Q_ \u001b[39m+\u001b[39m A\u001b[39m.\u001b[39mbmm(V_))\u001b[39m.\u001b[39msplit(Q\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m0\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='file:///home/santiago/anaconda3/envs/GNN/lib/python3.9/site-packages/torch_geometric/nn/glob/gmt.py?line=79'>80</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.83 GiB total capacity; 80.82 MiB already allocated; 45.81 MiB free; 88.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "iterator = create_loaders_nested(gnn_dataset, split=SPLITS, batch_size=BATCH_SIZE)\n",
    "MAE_outer = []\n",
    "counter = 0\n",
    "tot_runs = SPLITS * (SPLITS - 1)\n",
    "for outer in range(SPLITS):\n",
    "    MAE_inner = []\n",
    "    for inner in range(SPLITS - 1):\n",
    "        counter += 1\n",
    "        train_loader, val_loader, test_loader = next(iterator)\n",
    "        train_loader, val_loader, test_loader, mean_tv, std_tv = scale_target(train_loader, val_loader, test_loader,\n",
    "                                                                              mode=\"std\", verbose=False)\n",
    "        model = SantyxNet(dim=DIM, node_features=node_features).to(device)\n",
    "        optimizer = Adam(model.parameters(), lr=LR)  \n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=FACTOR, patience=PATIENCE, min_lr=MIN_LR)\n",
    "        loss_list = []  \n",
    "        train_list = [] \n",
    "        val_list = []   \n",
    "        test_list = []  \n",
    "        t0 = time.time()        \n",
    "        for epoch in range(1, EPOCHS+1):\n",
    "            lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            loss, train_MAE = train_loop(model, device, train_loader, optimizer, LOSS_FN)  \n",
    "            val_MAE = test_loop(model, val_loader, device, std_tv)            \n",
    "            scheduler.step(val_MAE)                                                       \n",
    "            test_MAE = test_loop(model, test_loader, device, std_tv)    \n",
    "            print('{}/{}-Epoch {:03d}: LR={:.7f}  Loss={:.6f}  Validation MAE: {:.6f} eV, '\n",
    "                'Test MAE: {:.6f} eV'.format(counter, tot_runs, epoch, lr, loss, val_MAE, test_MAE))\n",
    "            if epoch == EPOCHS:\n",
    "                MAE_inner.append(test_MAE)\n",
    "        print(\"Training time: {:.2f} s\".format(time.time() - t0))\n",
    "        loss_list.append(loss)\n",
    "        train_list.append(train_MAE * std_tv)\n",
    "        val_list.append(val_MAE)\n",
    "        test_list.append(test_MAE)\n",
    "        x, y = test_performance(\"{}-{}\".format(counter, tot_runs), model, \n",
    "                               train_loader, val_loader, test_loader, \n",
    "                               mean_tv, std_tv, \n",
    "                               SPLITS, EPOCHS, BATCH_SIZE, \n",
    "                               lr, MIN_LR, train_list, val_list, test_list)\n",
    "        with open(\"./NestedCrossValidation/Test.csv\", mode=\"a\") as outfile:\n",
    "            for a, b in zip(x, y):\n",
    "                outfile.write(f\"name,{a},ener{counter},{b}\\n\")\n",
    "        del model\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    MAE_outer.append(np.mean(MAE_inner))\n",
    "MAE = np.mean(MAE_outer)\n",
    "print(\"NESTED CROSS VALIDATION: MAE = {:.2f}\".format(MAE))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "929eb757b548c43686da14be07befd842a942a01eb9dc843387956885175000a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('GNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
