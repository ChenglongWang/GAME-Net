# Configuration file for setting the hyperparameter for GNN model training with train_GNN.py

[graph]

voronoi_tol = 0.25
scaling_factor = 1.5
second_order_nn = true

[train]  # Training related: All hyperparams except architecture-related ones

splits = 10                 # Initial splits of the starting dataset for train/val/test sets creation
test_set = true             # whether generate test set or just split among train/val
batch_size = 16             
epochs = 200
target_scaling = "std"      # Target scaling approach
loss_function = "mae"       # Loss function of the training
lr0 = 1e-3                  # Initial learning rate (lr)
patience = 5                # Patience of the lr-scheduler
factor = 0.7                # Decreasing factor of the lr-scheduler
minlr = 1e-7                # Minimum lr
betas = (0.9, 0.999)        # betas of adam optimizer
eps = 1e-9                  # adam eps for ensuring numerical stability of the algorithm
weight_decay = 0            
amsgrad = true              # Include amsgrad addition of adam

[architecture]  # All the hyperparameters defining the model architecture

dim = 128                   # dimension of the layers
sigma = ReLU     # Activation function
bias = false                # Whether allowing bias in all layer formulas
n_linear = 3                # Number of fully connected layers
n_conv = 3                  # Number of convolutional layers
conv_layer = SAGEConv       # Convolutional layer
adj_conv = true             # Whether adjust convolutional layer with fully connected one just before
conv_normalize = false      
conv_root_weight = false
pooling_layer = GraphMultisetTransformer  # Pooling layer
pool_ratio = 0.25                         # Poling ratio
pool_heads = 2                            
pool_seq = 1
pool_layer_norm = false

