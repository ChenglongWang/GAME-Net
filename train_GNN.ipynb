{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "from functions import create_loaders, scale_target, train_loop, test_loop\n",
    "from processed_datasets import FG_dataset, BM_dataset\n",
    "from nets import SantyxNet\n",
    "from post_training import create_model_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters of the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are all those parameters that are initialized before performing the model training (i.e., everything different from the model parameters). Hyperparameters can be categorized into model-related and process-related: Model-related hyperparameters are the activation function and the depth of the hidden layers, while the process-related ones are for example the batch size, the number of epochs and the loss function for the model optimization.\n",
    "\n",
    "N.B. The optimizer and the learning rate scheduler are potentially additional hyperparameters. In this project, for sake of simplicity, these algorithms are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = {}\n",
    "\n",
    "# Learning process\n",
    "HYPERPARAMS[\"test set\"] = True          # True=Generate train-val-test sets. False=Generate train-val (train with whole FG-dataset)\n",
    "HYPERPARAMS[\"splits\"] = 10              # Splits among which the dataset is partitioned to create train-val-test sets\n",
    "HYPERPARAMS[\"target scaling\"] = \"std\"   # Target scaling approach (std=standardization, norm=normalization, etc.)\n",
    "HYPERPARAMS[\"batch size\"] = 32           \n",
    "HYPERPARAMS[\"epochs\"] = 100              \n",
    "HYPERPARAMS[\"loss function\"] = torch.nn.functional.l1_loss   \n",
    "HYPERPARAMS[\"lr0\"] = 1e-3               # Initial learning rate (lr)\n",
    "HYPERPARAMS[\"patience\"] = 5             # Epochs with no improvement after which lr is reduced \n",
    "HYPERPARAMS[\"factor\"] = 0.7             # Decreasing factor applied by the lr scheduler\n",
    "HYPERPARAMS[\"minlr\"] = 1e-7             \n",
    "HYPERPARAMS[\"betas\"] = (0.9, 0.999)     # Adam optimizer: betas\n",
    "HYPERPARAMS[\"eps\"] = 1e-8               # Adam optimizer: eps\n",
    "HYPERPARAMS[\"weight decay\"] = 0         # Adam optimizer: weight decay\n",
    "HYPERPARAMS[\"amsgrad\"] = False          # Adam optimizer: amsgrad\n",
    "\n",
    "# Model structure\n",
    "HYPERPARAMS[\"dim\"] = 128                # Depth of the GNN layers\n",
    "HYPERPARAMS[\"sigma\"] = torch.nn.ReLU()  # Activation function of the GNN model\n",
    "HYPERPARAMS[\"bias\"] = True              # Bias presence in GNN layers\n",
    "HYPERPARAMS[\"conv normalize\"] = False   # GraphSAGE\n",
    "HYPERPARAMS[\"conv root weight\"] = True\n",
    "HYPERPARAMS[\"pool ratio\"] = 0.25        # Graph MultiSet Transormer\n",
    "HYPERPARAMS[\"pool heads\"] = 2\n",
    "HYPERPARAMS[\"pool seq\"] = [\"GMPool_G\", \"SelfAtt\", \"GMPool_I\"]\n",
    "HYPERPARAMS[\"pool layer norm\"] = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and target scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FG-dataset is split among the train, validation and test sets.\n",
    "The target scaling must be applied using parameters independent of the test set, as this would lead to \"data leakage\".\n",
    "Here, we apply the target scaling with the scale_target function, providing the optional parameter mode=\"std\" in order to apply standardization. Normalization can be applied optionally, providing the parameter mode=\"norm\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split (train/val): 90/10 %\n",
      "Training data = 2271 Validation data = 242 (Total = 2513)\n",
      "Target Scaling (Standardization) applied successfully\n",
      "(Train+Val) mean: -73.77 eV\n",
      "(Train+Val) standard deviation: 19.02 eV\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = create_loaders(FG_dataset,\n",
    "                                                       batch_size=HYPERPARAMS[\"batch size\"],\n",
    "                                                       split=HYPERPARAMS[\"splits\"], \n",
    "                                                       test=HYPERPARAMS[\"test set\"])\n",
    "train_loader, val_loader, test_loader, mean, std = scale_target(train_loader,\n",
    "                                                                val_loader,\n",
    "                                                                test_loader,\n",
    "                                                                mode=HYPERPARAMS[\"target scaling\"],\n",
    "                                                                test=HYPERPARAMS[\"test set\"])        \n",
    "BM_dataloader = DataLoader(BM_dataset)  # For testing extrapolation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device selection (GPU/CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a CUDA capable GPU is optimal for working with Deep Learning models, as its structure can be exploited in order to speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n",
      "Device name: NVIDIA GeForce MX450\n",
      "CUDA Version: 11.3\n",
      "CuDNN Version: 8200\n",
      "Python version: 3.9.12 \n",
      "Pytorch version: 1.10.1\n",
      "Pytorch Geometric version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Current device: {}\".format(device))\n",
    "if device == \"cuda\":\n",
    "    print(\"Device name: {}\".format(torch.cuda.get_device_name(0)))\n",
    "    print(\"CUDA Version: {}\".format(torch.version.cuda))\n",
    "    print(\"CuDNN Version: {}\".format(torch.backends.cudnn.version()))\n",
    "else: # cpu\n",
    "    print(\"Architecture: {}\".format(platform.machine()))\n",
    "    print(\"Platform: {}\".format(platform.platform()))\n",
    "    \n",
    "print(\"Python version: {}\".format(sys.version[:7]))\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"Pytorch Geometric version: {}\".format(torch_geometric.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN model instantiation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SantyxNet(dim=HYPERPARAMS[\"dim\"],\n",
    "                  sigma=HYPERPARAMS[\"sigma\"], \n",
    "                  bias=HYPERPARAMS[\"bias\"], \n",
    "                  conv_normalize=HYPERPARAMS[\"conv normalize\"], \n",
    "                  conv_root_weight=HYPERPARAMS[\"conv root weight\"], \n",
    "                  pool_ratio=HYPERPARAMS[\"pool ratio\"], \n",
    "                  pool_layer_norm=HYPERPARAMS[\"pool layer norm\"], \n",
    "                  pool_seq=HYPERPARAMS[\"pool seq\"], \n",
    "                  pool_heads=HYPERPARAMS[\"pool heads\"]).to(device)\n",
    "#summary(model)  # Print model building blocks (not architecture!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used optimizer for the training is Adam, algorithm for first-order gradient-based optimization of\n",
    "stochastic objective functions, based on adaptive estimates of lower-order mo-\n",
    "ments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=HYPERPARAMS[\"lr0\"], \n",
    "                             betas=HYPERPARAMS[\"betas\"], \n",
    "                             eps=HYPERPARAMS[\"eps\"], \n",
    "                             weight_decay=HYPERPARAMS[\"weight decay\"], \n",
    "                             amsgrad=HYPERPARAMS[\"amsgrad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate (LR) Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps steering the learning rate during the training, providing faster convergence and higher accuracy. The used scheduler is the \"Reduce On Loss Plateau Decay\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode='min',\n",
    "                                                       factor=HYPERPARAMS[\"factor\"],\n",
    "                                                       patience=HYPERPARAMS[\"patience\"],\n",
    "                                                       min_lr=HYPERPARAMS[\"minlr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: LR=0.0010000  Train MAE: 25.223948 eV  Validation MAE: 5.685780 eV \n",
      "Epoch 002: LR=0.0010000  Train MAE: 4.870587 eV  Validation MAE: 3.906790 eV \n",
      "Epoch 003: LR=0.0010000  Train MAE: 5.937455 eV  Validation MAE: 2.404832 eV \n",
      "Epoch 004: LR=0.0010000  Train MAE: 4.104703 eV  Validation MAE: 2.389748 eV \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/santiago/Desktop/GNN/train_GNN.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/train_GNN.ipynb#ch0000020?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, HYPERPARAMS[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/train_GNN.ipynb#ch0000020?line=7'>8</a>\u001b[0m     lr \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/train_GNN.ipynb#ch0000020?line=8'>9</a>\u001b[0m     loss, train_MAE \u001b[39m=\u001b[39m train_loop(model, device, train_loader, optimizer, HYPERPARAMS[\u001b[39m\"\u001b[39;49m\u001b[39mloss function\u001b[39;49m\u001b[39m\"\u001b[39;49m])  \u001b[39m# Run epoch and update params\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/train_GNN.ipynb#ch0000020?line=9'>10</a>\u001b[0m     val_MAE \u001b[39m=\u001b[39m test_loop(model, val_loader, device, std)                                                 \u001b[39m# Run epoch on validation set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/santiago/Desktop/GNN/train_GNN.ipynb#ch0000020?line=10'>11</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep(val_MAE)                                                                          \u001b[39m# Adjust lr based on val. error\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GNN/functions.py:579\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, device, train_loader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(model(batch), batch\u001b[39m.\u001b[39my)\n\u001b[1;32m    578\u001b[0m mae \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39ml1_loss(model(batch), batch\u001b[39m.\u001b[39my)    \u001b[39m# For comparison with val/test data\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()                           \u001b[39m# Compute gradient of loss function wrt parameters\u001b[39;00m\n\u001b[1;32m    580\u001b[0m loss_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch\u001b[39m.\u001b[39mnum_graphs\n\u001b[1;32m    581\u001b[0m mae_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m mae\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch\u001b[39m.\u001b[39mnum_graphs\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/GNN/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_error = None\n",
    "loss_list = []  # Store loss function trend during training\n",
    "train_list = [] # Store training score during training\n",
    "val_list = []   # Store validation score during training\n",
    "test_list = []  # Store test score during training\n",
    "t0 = time.time()\n",
    "for epoch in range(1, HYPERPARAMS[\"epochs\"]+1):\n",
    "    lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss, train_MAE = train_loop(model, device, train_loader, optimizer, HYPERPARAMS[\"loss function\"])  # Run epoch and update params\n",
    "    val_MAE = test_loop(model, val_loader, device, std)                                                 # Run epoch on validation set\n",
    "    lr_scheduler.step(val_MAE)                                                                          # Adjust lr based on val. error\n",
    "    \n",
    "    if HYPERPARAMS[\"test set\"]:\n",
    "        test_MAE = test_loop(model, BM_dataloader, device, std, mean, scaled_graph_label=False)                                           # Run epoch on test set\n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.4f} eV  Validation MAE: {:.4f} eV '             \n",
    "              'Test MAE: {:.4f} eV'.format(epoch, lr, train_MAE*std, val_MAE, test_MAE))\n",
    "    else:\n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.6f} eV  Validation MAE: {:.6f} eV '\n",
    "              .format(epoch, lr, train_MAE*std, val_MAE))\n",
    "\n",
    "    loss_list.append(loss)\n",
    "    train_list.append(train_MAE * std)\n",
    "    val_list.append(val_MAE)\n",
    "    if HYPERPARAMS[\"test set\"]:\n",
    "        test_list.append(test_MAE)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"device: {}    Training time: {:.2f} s\".format(device, time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 40\n",
      "Mean Absolute Error = 11.123753547668457 eV\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.123753547668457"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loop(model, BM_dataloader, device, std, mean, scaled_graph_label=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method-wrapper '__repr__' of DataLoader object at 0x7f6687537970>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_err_BM = [] \n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "for sample in BM_dataloader.dataset:\n",
    "    E_DFT = sample.y\n",
    "    sample.y_GNN = model(sample).item() * std + mean\n",
    "    E_GNN = sample.y_GNN\n",
    "    abs_err_BM.append(abs(E_GNN - E_DFT))\n",
    "BM_MAE = np.mean(abs_err_BM)\n",
    "for graph in BM_dataloader.dataset:\n",
    "    print(\"{} DFT = {:.2f} eV, GNN = {:.2f} eV\".format(graph.formula, graph.y, graph.y_GNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_report(\"test\",\n",
    "                    model,\n",
    "                    (train_loader, val_loader, test_loader), \n",
    "                    (mean, std), \n",
    "                    HYPERPARAMS, \n",
    "                    (train_list, val_list, test_list))\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.load(\"./Models/test/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "929eb757b548c43686da14be07befd842a942a01eb9dc843387956885175000a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('GNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
