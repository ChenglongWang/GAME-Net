{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "from functions import create_loaders, scale_target, train_loop, test_loop\n",
    "from processed_datasets import FG_dataset, BM_dataset\n",
    "from nets import SantyxNet\n",
    "from post_training import create_model_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters of the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are all those parameters that are initialized before performing the model training (i.e., everything different from the model parameters). Hyperparameters can be categorized into model-related and process-related: Model-related hyperparameters are the activation function and the depth of the hidden layers, while the process-related ones are for example the batch size, the number of epochs and the loss function for the model optimization.\n",
    "\n",
    "N.B. The optimizer and the learning rate scheduler are potentially additional hyperparameters. In this project, for sake of simplicity, these algorithms are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = {}\n",
    "\n",
    "# Learning process\n",
    "HYPERPARAMS[\"test set\"] = True          # True=Generate train-val-test sets. False=Generate train-val (train with whole FG-dataset)\n",
    "HYPERPARAMS[\"splits\"] = 10              # Splits among which the dataset is partitioned to create train-val-test sets\n",
    "HYPERPARAMS[\"target scaling\"] = \"std\"   # Target scaling approach (std=standardization, norm=normalization, etc.)\n",
    "HYPERPARAMS[\"batch size\"] = 32           \n",
    "HYPERPARAMS[\"epochs\"] = 100              \n",
    "HYPERPARAMS[\"loss function\"] = torch.nn.functional.l1_loss   \n",
    "HYPERPARAMS[\"lr0\"] = 1e-3               # Initial learning rate (lr)\n",
    "HYPERPARAMS[\"patience\"] = 5             # Epochs with no improvement after which lr is reduced \n",
    "HYPERPARAMS[\"factor\"] = 0.7             # Decreasing factor applied by the lr scheduler\n",
    "HYPERPARAMS[\"minlr\"] = 1e-7             \n",
    "HYPERPARAMS[\"betas\"] = (0.9, 0.999)     # Adam optimizer: betas\n",
    "HYPERPARAMS[\"eps\"] = 1e-8               # Adam optimizer: eps\n",
    "HYPERPARAMS[\"weight decay\"] = 0         # Adam optimizer: weight decay\n",
    "HYPERPARAMS[\"amsgrad\"] = False          # Adam optimizer: amsgrad\n",
    "\n",
    "# Model structure\n",
    "HYPERPARAMS[\"dim\"] = 128                # Depth of the GNN layers\n",
    "HYPERPARAMS[\"sigma\"] = torch.nn.ReLU()  # Activation function of the GNN model\n",
    "HYPERPARAMS[\"bias\"] = True              # Bias presence in GNN layers\n",
    "HYPERPARAMS[\"conv normalize\"] = False   # GraphSAGE\n",
    "HYPERPARAMS[\"conv root weight\"] = True\n",
    "HYPERPARAMS[\"pool ratio\"] = 0.25        # Graph MultiSet Transormer\n",
    "HYPERPARAMS[\"pool heads\"] = 2\n",
    "HYPERPARAMS[\"pool seq\"] = [\"GMPool_G\", \"SelfAtt\", \"GMPool_I\"]\n",
    "HYPERPARAMS[\"pool layer norm\"] = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and target scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FG-dataset is split among the train, validation and test sets.\n",
    "The target scaling must be applied using parameters independent of the test set, as this would lead to \"data leakage\".\n",
    "Here, we apply the target scaling with the scale_target function, providing the optional parameter mode=\"std\" in order to apply standardization. Normalization can be applied optionally, providing the parameter mode=\"norm\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_loaders(FG_dataset,\n",
    "                                                       batch_size=HYPERPARAMS[\"batch size\"],\n",
    "                                                       split=HYPERPARAMS[\"splits\"], \n",
    "                                                       test=HYPERPARAMS[\"test set\"])\n",
    "train_loader, val_loader, test_loader, mean, std = scale_target(train_loader,\n",
    "                                                                val_loader,\n",
    "                                                                test_loader,\n",
    "                                                                mode=HYPERPARAMS[\"target scaling\"],\n",
    "                                                                test=HYPERPARAMS[\"test set\"])        \n",
    "BM_dataloader = DataLoader(BM_dataset)  # For testing extrapolation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device selection (GPU/CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a CUDA capable GPU is optimal for working with Deep Learning models, as its structure can be exploited in order to speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Current device: {}\".format(device))\n",
    "if device == \"cuda\":\n",
    "    print(\"Device name: {}\".format(torch.cuda.get_device_name(0)))\n",
    "    print(\"CUDA Version: {}\".format(torch.version.cuda))\n",
    "    print(\"CuDNN Version: {}\".format(torch.backends.cudnn.version()))\n",
    "else: # cpu\n",
    "    print(\"Architecture: {}\".format(platform.machine()))\n",
    "    print(\"Platform: {}\".format(platform.platform()))\n",
    "    \n",
    "print(\"Python version: {}\".format(sys.version[:7]))\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"Pytorch Geometric version: {}\".format(torch_geometric.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN model instantiation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SantyxNet(dim=HYPERPARAMS[\"dim\"],\n",
    "                  sigma=HYPERPARAMS[\"sigma\"], \n",
    "                  bias=HYPERPARAMS[\"bias\"], \n",
    "                  conv_normalize=HYPERPARAMS[\"conv normalize\"], \n",
    "                  conv_root_weight=HYPERPARAMS[\"conv root weight\"], \n",
    "                  pool_ratio=HYPERPARAMS[\"pool ratio\"], \n",
    "                  pool_layer_norm=HYPERPARAMS[\"pool layer norm\"], \n",
    "                  pool_seq=HYPERPARAMS[\"pool seq\"], \n",
    "                  pool_heads=HYPERPARAMS[\"pool heads\"]).to(device)\n",
    "#summary(model)  # Print model building blocks (not architecture!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used optimizer for the training is Adam, algorithm for first-order gradient-based optimization of\n",
    "stochastic objective functions, based on adaptive estimates of lower-order mo-\n",
    "ments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=HYPERPARAMS[\"lr0\"], \n",
    "                             betas=HYPERPARAMS[\"betas\"], \n",
    "                             eps=HYPERPARAMS[\"eps\"], \n",
    "                             weight_decay=HYPERPARAMS[\"weight decay\"], \n",
    "                             amsgrad=HYPERPARAMS[\"amsgrad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate (LR) Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps steering the learning rate during the training, providing faster convergence and higher accuracy. The used scheduler is the \"Reduce On Loss Plateau Decay\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode='min',\n",
    "                                                       factor=HYPERPARAMS[\"factor\"],\n",
    "                                                       patience=HYPERPARAMS[\"patience\"],\n",
    "                                                       min_lr=HYPERPARAMS[\"minlr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_error = None\n",
    "loss_list = []  # Store loss function trend during training\n",
    "train_list = [] # Store training score during training\n",
    "val_list = []   # Store validation score during training\n",
    "test_list = []  # Store test score during training\n",
    "t0 = time.time()\n",
    "for epoch in range(1, HYPERPARAMS[\"epochs\"]+1):\n",
    "    lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss, train_MAE = train_loop(model, device, train_loader, optimizer, HYPERPARAMS[\"loss function\"])  # Run epoch and update params\n",
    "    val_MAE = test_loop(model, val_loader, device, std)                                                 # Run epoch on validation set\n",
    "    lr_scheduler.step(val_MAE)                                                                          # Adjust lr based on val. error\n",
    "    \n",
    "    if HYPERPARAMS[\"test set\"]:\n",
    "        test_MAE = test_loop(model, BM_dataloader, device, std, mean, scaled_graph_label=False)                                           # Run epoch on test set\n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.4f} eV  Validation MAE: {:.4f} eV '             \n",
    "              'Test MAE: {:.4f} eV'.format(epoch, lr, train_MAE*std, val_MAE, test_MAE))\n",
    "    else:\n",
    "        print('Epoch {:03d}: LR={:.7f}  Train MAE: {:.6f} eV  Validation MAE: {:.6f} eV '\n",
    "              .format(epoch, lr, train_MAE*std, val_MAE))\n",
    "\n",
    "    loss_list.append(loss)\n",
    "    train_list.append(train_MAE * std)\n",
    "    val_list.append(val_MAE)\n",
    "    if HYPERPARAMS[\"test set\"]:\n",
    "        test_list.append(test_MAE)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"device: {}    Training time: {:.2f} s\".format(device, time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(model, BM_dataloader, device, std, mean, scaled_graph_label=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_err_BM = [] \n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "for sample in BM_dataloader.dataset:\n",
    "    E_DFT = sample.y\n",
    "    sample.y_GNN = model(sample).item() * std + mean\n",
    "    E_GNN = sample.y_GNN\n",
    "    abs_err_BM.append(abs(E_GNN - E_DFT))\n",
    "BM_MAE = np.mean(abs_err_BM)\n",
    "for graph in BM_dataloader.dataset:\n",
    "    print(\"{} DFT = {:.2f} eV, GNN = {:.2f} eV\".format(graph.formula, graph.y, graph.y_GNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_report(\"test\",\n",
    "                    model,\n",
    "                    (train_loader, val_loader, test_loader), \n",
    "                    (mean, std), \n",
    "                    HYPERPARAMS, \n",
    "                    (train_list, val_list, test_list))\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.load(\"./Models/test/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "929eb757b548c43686da14be07befd842a942a01eb9dc843387956885175000a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('GNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
